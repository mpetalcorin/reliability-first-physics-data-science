{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f58bbe-05c9-4c17-96f8-b444fa9792a2",
   "metadata": {},
   "source": [
    "# Shift-Aware Time-Series Modelling and Bayesian Optimisation for Physics-Style Discovery\n",
    "\n",
    "## Summary\n",
    "This script presents a fully reproducible, decision-oriented data science workflow tailored to physics, astronomy, and mathematics applications where data are costly, distribution shift is common, and uncertainty must be operationally meaningful. Using simulated light-curve time series, the benchmark evaluates regression and classification under random splits, leakage-resistant group splits, and explicit out-of-distribution tests, showing that performance and calibration can appear optimistic under naive splitting but degrade under more realistic generalisation and shift. Reliability diagrams and risk–coverage curves demonstrate how calibration and selective prediction enable safer, uncertainty-aware deployment. The work then links prediction to experiment design by demonstrating Gaussian process Bayesian optimisation for an expensive simulator, and extends to multi-objective Bayesian optimisation that yields Pareto trade-offs prioritised by hypervolume contribution and ε-constraint decision rules. Overall, the study frames modern ML not only as prediction, but as reliable decision support under uncertainty and limited evaluation budgets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e8617-1a3e-4b20-80bf-212780462e8f",
   "metadata": {},
   "source": [
    "# Reliable Scientific Data Science for Astronomy: Exoplanet Transit Detection + Parameter Estimation under Shift\n",
    "\n",
    "## What this script demonstrates (end-to-end, reproducible):\n",
    "    1) Physics-informed synthetic dataset generation (light curves with transits + stellar variability)\n",
    "    2) Feature engineering (time-domain + FFT spectral features)\n",
    "    3) Two tasks:\n",
    "         - T1: Classification: \"transit present?\" (binary)\n",
    "         - T2: Regression: estimate orbital period (days) when transit exists\n",
    "    4) Realistic evaluation:\n",
    "         - Random split (optimistic, leakage risk)\n",
    "         - Group split by star_id (more realistic generalisation)\n",
    "    5) OOD testing:\n",
    "         - Changed noise and cadence (distribution shift)\n",
    "    6) Uncertainty:\n",
    "         - Probability calibration (isotonic/sigmoid)\n",
    "         - Reliability diagrams + Brier score\n",
    "         - Risk–coverage selective prediction curves (abstain on uncertain samples)\n",
    "    7) Reproducible outputs:\n",
    "         - Figures and CSV tables saved to --outdir\n",
    "\n",
    "## Dependencies:\n",
    "    numpy, pandas, matplotlib, scikit-learn\n",
    "\n",
    "## Run:\n",
    "    python uh_data_science_physics_astronomy_poc.py --outdir outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79c70bb-e5af-4dc2-b0b4-ee9212b633bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Science for Astronomy PoC ===\n",
      "Outputs saved to: /Users/petalc01/Data Science Hertfordshire/uh_poc_outputs\n",
      "\n",
      "Classification (Transit detection)\n",
      "  Random split: {'roc_auc': 0.851017441860465, 'avg_precision': 0.9107412808532469, 'accuracy': 0.7733333333333333, 'f1': 0.7733333333333333, 'brier': 0.16322909565075674}\n",
      "  Group split : {'roc_auc': 0.904074074074074, 'avg_precision': 0.9347588427951816, 'accuracy': 0.8666666666666667, 'f1': 0.8809523809523809, 'brier': 0.10766760867462419}\n",
      "  OOD (rand)  : {'roc_auc': 0.8049510701997408, 'avg_precision': 0.8059992001204431, 'accuracy': 0.5733333333333334, 'f1': 0.7023255813953488, 'brier': 0.35099630482449795}\n",
      "  OOD (group) : {'roc_auc': 0.6715224094016712, 'avg_precision': 0.6490832254924903, 'accuracy': 0.5766666666666667, 'f1': 0.703962703962704, 'brier': 0.35228743917735567}\n",
      "\n",
      "Regression (Period estimation, positives only)\n",
      "  Random split: {'mae': 2.614105589001194, 'rmse': 2.970435500428323, 'r2': 0.24950818565430355}\n",
      "  Group split : {'mae': 2.6141055890011935, 'rmse': 2.970435500428323, 'r2': 0.24950818565430366}\n",
      "\n",
      "Key figures written to outputs/figures/\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    brier_score_loss,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "\n",
    "# Utilities (reproducibility)\n",
    "\n",
    "def set_seed(seed: int = 42) -> np.random.Generator:\n",
    "    return np.random.default_rng(seed)\n",
    "\n",
    "\n",
    "def ensure_outdir(outdir: str) -> str:\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    figdir = os.path.join(outdir, \"figures\")\n",
    "    os.makedirs(figdir, exist_ok=True)\n",
    "    tabdir = os.path.join(outdir, \"tables\")\n",
    "    os.makedirs(tabdir, exist_ok=True)\n",
    "    return outdir\n",
    "\n",
    "\n",
    "def save_json(obj: Dict, path: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "\n",
    "# Synthetic astronomy generator\n",
    "\n",
    "@dataclass\n",
    "class SimConfig:\n",
    "    n_stars: int = 300\n",
    "    min_obs: int = 220\n",
    "    max_obs: int = 320\n",
    "    transit_fraction: float = 0.55  # fraction of stars with a planet transit\n",
    "    duration_days: float = 30.0     # observation window length\n",
    "    cadence_min: float = 0.08       # min sampling interval (days)\n",
    "    cadence_max: float = 0.20       # max sampling interval (days)\n",
    "    base_noise: float = 0.0009      # baseline photometric noise\n",
    "    variability_amp: Tuple[float, float] = (0.0005, 0.0035)\n",
    "    variability_freq: Tuple[float, float] = (0.02, 0.45)  # cycles per day\n",
    "    transit_depth: Tuple[float, float] = (0.001, 0.02)    # fractional flux drop\n",
    "    transit_width: Tuple[float, float] = (0.05, 0.35)     # days\n",
    "    period_range: Tuple[float, float] = (1.2, 12.0)       # days\n",
    "    jitter_time: float = 0.01                              # time jitter\n",
    "\n",
    "\n",
    "def simulate_light_curve(\n",
    "    rng: np.random.Generator,\n",
    "    n_obs: int,\n",
    "    duration_days: float,\n",
    "    cadence_range: Tuple[float, float],\n",
    "    noise_sigma: float,\n",
    "    variability_amp_range: Tuple[float, float],\n",
    "    variability_freq_range: Tuple[float, float],\n",
    "    has_transit: bool,\n",
    "    period_range: Tuple[float, float],\n",
    "    transit_depth_range: Tuple[float, float],\n",
    "    transit_width_range: Tuple[float, float],\n",
    "    time_jitter: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Simulate a \"star\" light curve:\n",
    "        flux(t) = 1 + variability(t) - transit(t) + Gaussian noise\n",
    "\n",
    "    Transit is modelled as repeated Gaussian dips at times around multiples of the orbital period.\n",
    "    \"\"\"\n",
    "    # Irregular sampling times\n",
    "    cadence = rng.uniform(cadence_range[0], cadence_range[1])\n",
    "    t = np.arange(0.0, duration_days, cadence)\n",
    "    if len(t) > n_obs:\n",
    "        t = t[:n_obs]\n",
    "    else:\n",
    "        # if too few points, pad by re-sampling a slightly different cadence\n",
    "        while len(t) < n_obs:\n",
    "            cadence2 = rng.uniform(cadence_range[0], cadence_range[1])\n",
    "            t2 = np.arange(0.0, duration_days, cadence2)\n",
    "            t = np.unique(np.concatenate([t, t2]))\n",
    "            t = t[:n_obs]\n",
    "\n",
    "    # Add jitter\n",
    "    t = t + rng.normal(0.0, time_jitter, size=t.shape)\n",
    "    t = np.clip(t, 0.0, duration_days)\n",
    "    t = np.sort(t)\n",
    "\n",
    "    # Stellar variability (sinusoid + second harmonic)\n",
    "    amp = rng.uniform(*variability_amp_range)\n",
    "    freq = rng.uniform(*variability_freq_range)\n",
    "    phase = rng.uniform(0, 2 * np.pi)\n",
    "    variability = amp * np.sin(2 * np.pi * freq * t + phase) + 0.35 * amp * np.sin(2 * np.pi * (2*freq) * t + 0.7*phase)\n",
    "\n",
    "    # Transit parameters (if present)\n",
    "    if has_transit:\n",
    "        period = rng.uniform(*period_range)\n",
    "        depth = rng.uniform(*transit_depth_range)\n",
    "        width = rng.uniform(*transit_width_range)\n",
    "        t0 = rng.uniform(0.0, period)  # random epoch within first period\n",
    "\n",
    "        # Create dips: sum of Gaussians at each transit center\n",
    "        # (simple but effective \"physics-ish\" proxy for dips)\n",
    "        centers = t0 + np.arange(-2, int(duration_days / period) + 3) * period\n",
    "        transit = np.zeros_like(t)\n",
    "        for c in centers:\n",
    "            transit += depth * np.exp(-0.5 * ((t - c) / width) ** 2)\n",
    "    else:\n",
    "        period = np.nan\n",
    "        depth = np.nan\n",
    "        width = np.nan\n",
    "        t0 = np.nan\n",
    "        transit = np.zeros_like(t)\n",
    "\n",
    "    # Observed flux\n",
    "    noise = rng.normal(0.0, noise_sigma, size=t.shape)\n",
    "    flux = 1.0 + variability - transit + noise\n",
    "\n",
    "    meta = {\n",
    "        \"period\": period,\n",
    "        \"depth\": depth,\n",
    "        \"width\": width,\n",
    "        \"variability_amp\": amp,\n",
    "        \"variability_freq\": freq,\n",
    "        \"noise_sigma\": noise_sigma,\n",
    "    }\n",
    "    return t, flux, meta\n",
    "\n",
    "\n",
    "def simulate_dataset(rng: np.random.Generator, cfg: SimConfig) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        curves_df: long-form time series with columns [star_id, t, flux]\n",
    "        labels_df: per-star table with targets/metadata\n",
    "    \"\"\"\n",
    "    curves = []\n",
    "    labels = []\n",
    "\n",
    "    for star_id in range(cfg.n_stars):\n",
    "        n_obs = int(rng.integers(cfg.min_obs, cfg.max_obs + 1))\n",
    "        has_transit = rng.random() < cfg.transit_fraction\n",
    "\n",
    "        t, flux, meta = simulate_light_curve(\n",
    "            rng=rng,\n",
    "            n_obs=n_obs,\n",
    "            duration_days=cfg.duration_days,\n",
    "            cadence_range=(cfg.cadence_min, cfg.cadence_max),\n",
    "            noise_sigma=cfg.base_noise * rng.uniform(0.85, 1.25),\n",
    "            variability_amp_range=cfg.variability_amp,\n",
    "            variability_freq_range=cfg.variability_freq,\n",
    "            has_transit=has_transit,\n",
    "            period_range=cfg.period_range,\n",
    "            transit_depth_range=cfg.transit_depth,\n",
    "            transit_width_range=cfg.transit_width,\n",
    "            time_jitter=cfg.jitter_time,\n",
    "        )\n",
    "\n",
    "        curves.append(pd.DataFrame({\"star_id\": star_id, \"t\": t, \"flux\": flux}))\n",
    "        labels.append(\n",
    "            {\n",
    "                \"star_id\": star_id,\n",
    "                \"has_transit\": int(has_transit),\n",
    "                \"period_days\": float(meta[\"period\"]) if has_transit else np.nan,\n",
    "                \"transit_depth\": float(meta[\"depth\"]) if has_transit else np.nan,\n",
    "                \"transit_width\": float(meta[\"width\"]) if has_transit else np.nan,\n",
    "                \"variability_amp\": float(meta[\"variability_amp\"]),\n",
    "                \"variability_freq\": float(meta[\"variability_freq\"]),\n",
    "                \"noise_sigma\": float(meta[\"noise_sigma\"]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    curves_df = pd.concat(curves, ignore_index=True)\n",
    "    labels_df = pd.DataFrame(labels).sort_values(\"star_id\").reset_index(drop=True)\n",
    "    return curves_df, labels_df\n",
    "\n",
    "\n",
    "# Feature engineering\n",
    "\n",
    "def extract_features_for_star(t: np.ndarray, flux: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Physics/data-science friendly features:\n",
    "        - time-domain robust stats\n",
    "        - simple \"transit-likeness\" metrics\n",
    "        - FFT spectral features (dominant frequency power)\n",
    "    \"\"\"\n",
    "    # Basic stats\n",
    "    x = flux.astype(np.float64)\n",
    "    median = float(np.median(x))\n",
    "    mad = float(np.median(np.abs(x - median))) + 1e-12  # robust scale\n",
    "\n",
    "    # Detrend lightly (remove median)\n",
    "    y = x - median\n",
    "\n",
    "    # Time-domain moments\n",
    "    std = float(np.std(y))\n",
    "    p01 = float(np.quantile(y, 0.01))\n",
    "    p05 = float(np.quantile(y, 0.05))\n",
    "    p95 = float(np.quantile(y, 0.95))\n",
    "    p99 = float(np.quantile(y, 0.99))\n",
    "\n",
    "    # \"Dip\" emphasis: how strong are negative excursions?\n",
    "    neg_tail = float(np.mean(y[y < np.quantile(y, 0.10)])) if np.any(y < np.quantile(y, 0.10)) else 0.0\n",
    "    dip_score = float((np.abs(p01) + np.abs(p05)) / (std + 1e-9))\n",
    "\n",
    "    # Autocorrelation at small lag (rough periodicity signal)\n",
    "    # Interpolate to uniform grid for rough ACF and FFT\n",
    "    # Use a fixed grid length\n",
    "    n_grid = 256\n",
    "    t_min, t_max = float(np.min(t)), float(np.max(t))\n",
    "    if t_max - t_min < 1e-6:\n",
    "        # degenerate\n",
    "        return {\n",
    "            \"median\": median,\n",
    "            \"mad\": mad,\n",
    "            \"std\": std,\n",
    "            \"p01\": p01,\n",
    "            \"p05\": p05,\n",
    "            \"p95\": p95,\n",
    "            \"p99\": p99,\n",
    "            \"neg_tail\": neg_tail,\n",
    "            \"dip_score\": dip_score,\n",
    "            \"acf1\": 0.0,\n",
    "            \"acf5\": 0.0,\n",
    "            \"fft_peak_freq\": 0.0,\n",
    "            \"fft_peak_power\": 0.0,\n",
    "            \"fft_power_ratio\": 0.0,\n",
    "        }\n",
    "\n",
    "    grid_t = np.linspace(t_min, t_max, n_grid)\n",
    "    grid_y = np.interp(grid_t, t, y)\n",
    "\n",
    "    # ACF at lag 1 and 5\n",
    "    def autocorr(a: np.ndarray, lag: int) -> float:\n",
    "        if lag >= len(a):\n",
    "            return 0.0\n",
    "        a0 = a[:-lag]\n",
    "        a1 = a[lag:]\n",
    "        denom = (np.std(a0) * np.std(a1) + 1e-12)\n",
    "        return float(np.mean((a0 - np.mean(a0)) * (a1 - np.mean(a1))) / denom)\n",
    "\n",
    "    acf1 = autocorr(grid_y, 1)\n",
    "    acf5 = autocorr(grid_y, 5)\n",
    "\n",
    "    # FFT\n",
    "    # Remove mean, use rfft\n",
    "    gy = grid_y - np.mean(grid_y)\n",
    "    fft = np.fft.rfft(gy)\n",
    "    power = (np.abs(fft) ** 2).astype(np.float64)\n",
    "    freqs = np.fft.rfftfreq(n_grid, d=(grid_t[1] - grid_t[0]))\n",
    "\n",
    "    # Ignore DC component\n",
    "    if len(power) > 1:\n",
    "        power_no_dc = power.copy()\n",
    "        power_no_dc[0] = 0.0\n",
    "        peak_idx = int(np.argmax(power_no_dc))\n",
    "        fft_peak_freq = float(freqs[peak_idx])\n",
    "        fft_peak_power = float(power_no_dc[peak_idx])\n",
    "        total_power = float(np.sum(power_no_dc) + 1e-12)\n",
    "        fft_power_ratio = float(fft_peak_power / total_power)\n",
    "    else:\n",
    "        fft_peak_freq, fft_peak_power, fft_power_ratio = 0.0, 0.0, 0.0\n",
    "\n",
    "    return {\n",
    "        \"median\": median,\n",
    "        \"mad\": mad,\n",
    "        \"std\": std,\n",
    "        \"p01\": p01,\n",
    "        \"p05\": p05,\n",
    "        \"p95\": p95,\n",
    "        \"p99\": p99,\n",
    "        \"neg_tail\": neg_tail,\n",
    "        \"dip_score\": dip_score,\n",
    "        \"acf1\": acf1,\n",
    "        \"acf5\": acf5,\n",
    "        \"fft_peak_freq\": fft_peak_freq,\n",
    "        \"fft_peak_power\": fft_peak_power,\n",
    "        \"fft_power_ratio\": fft_power_ratio,\n",
    "    }\n",
    "\n",
    "\n",
    "def build_feature_table(curves_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert long-form time series to per-star feature table.\n",
    "    \"\"\"\n",
    "    feats = []\n",
    "    for star_id, sub in curves_df.groupby(\"star_id\", sort=True):\n",
    "        t = sub[\"t\"].to_numpy()\n",
    "        flux = sub[\"flux\"].to_numpy()\n",
    "        f = extract_features_for_star(t, flux)\n",
    "        f[\"star_id\"] = int(star_id)\n",
    "        feats.append(f)\n",
    "\n",
    "    feat_df = pd.DataFrame(feats).sort_values(\"star_id\").reset_index(drop=True)\n",
    "    return feat_df\n",
    "\n",
    "\n",
    "# Evaluation helpers\n",
    "\n",
    "def classification_metrics(y_true: np.ndarray, proba: np.ndarray, thresh: float = 0.5) -> Dict[str, float]:\n",
    "    y_pred = (proba >= thresh).astype(int)\n",
    "    return {\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, proba)),\n",
    "        \"avg_precision\": float(average_precision_score(y_true, proba)),\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred)),\n",
    "        \"brier\": float(brier_score_loss(y_true, proba)),\n",
    "    }\n",
    "\n",
    "\n",
    "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return {\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"rmse\": float(rmse),\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_reliability(y_true: np.ndarray, proba: np.ndarray, title: str, outpath: str) -> None:\n",
    "    frac_pos, mean_pred = calibration_curve(y_true, proba, n_bins=10, strategy=\"uniform\")\n",
    "\n",
    "    plt.figure(figsize=(6.4, 5.2))\n",
    "    plt.plot(mean_pred, frac_pos, marker=\"o\")\n",
    "    plt.plot([0, 1], [0, 1])\n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"Fraction of positives\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def risk_coverage_curve(y_true: np.ndarray, proba: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Selective prediction: abstain on low-confidence points.\n",
    "    Risk = 1 - accuracy on retained subset\n",
    "    Coverage = fraction retained\n",
    "    \"\"\"\n",
    "    # confidence = max(p, 1-p)\n",
    "    conf = np.maximum(proba, 1.0 - proba)\n",
    "    order = np.argsort(-conf)  # high confidence first\n",
    "\n",
    "    y_true_ord = y_true[order]\n",
    "    proba_ord = proba[order]\n",
    "    pred_ord = (proba_ord >= 0.5).astype(int)\n",
    "\n",
    "    risks = []\n",
    "    coverages = []\n",
    "    for k in range(10, len(y_true_ord) + 1, max(10, len(y_true_ord) // 60)):\n",
    "        yt = y_true_ord[:k]\n",
    "        yp = pred_ord[:k]\n",
    "        acc = accuracy_score(yt, yp)\n",
    "        risk = 1.0 - acc\n",
    "        coverage = k / len(y_true_ord)\n",
    "        risks.append(risk)\n",
    "        coverages.append(coverage)\n",
    "\n",
    "    return np.array(coverages), np.array(risks)\n",
    "\n",
    "\n",
    "def plot_risk_coverage(y_true: np.ndarray, proba: np.ndarray, title: str, outpath: str) -> None:\n",
    "    cov, risk = risk_coverage_curve(y_true, proba)\n",
    "    plt.figure(figsize=(6.4, 5.2))\n",
    "    plt.plot(cov, risk, marker=\"o\")\n",
    "    plt.xlabel(\"Coverage (fraction predicted)\")\n",
    "    plt.ylabel(\"Risk (1 - accuracy)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# OOD dataset generator\n",
    "\n",
    "def simulate_ood_dataset(rng: np.random.Generator, base_cfg: SimConfig) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    OOD shift:\n",
    "        - higher noise\n",
    "        - slightly different cadence regime\n",
    "        - stronger variability amplitude distribution\n",
    "    \"\"\"\n",
    "    cfg = SimConfig(**vars(base_cfg))\n",
    "    cfg.base_noise *= 2.1\n",
    "    cfg.cadence_min *= 0.7\n",
    "    cfg.cadence_max *= 1.35\n",
    "    cfg.variability_amp = (base_cfg.variability_amp[0] * 1.2, base_cfg.variability_amp[1] * 1.6)\n",
    "\n",
    "    return simulate_dataset(rng, cfg)\n",
    "\n",
    "\n",
    "# Main experiment\n",
    "\n",
    "def run_experiment(\n",
    "    outdir: str,\n",
    "    seed: int = 42,\n",
    "    n_stars: int = 300,\n",
    "    model_kind: str = \"rf\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    model_kind:\n",
    "        - \"lr\": LogisticRegression + Ridge\n",
    "        - \"rf\": RandomForest\n",
    "    \"\"\"\n",
    "    rng = set_seed(seed)\n",
    "    ensure_outdir(outdir)\n",
    "\n",
    "    # 1) Simulate train-like dataset\n",
    "    cfg = SimConfig(n_stars=n_stars)\n",
    "    curves_df, labels_df = simulate_dataset(rng, cfg)\n",
    "\n",
    "    # Save raw dataset summaries\n",
    "    labels_df.to_csv(os.path.join(outdir, \"tables\", \"labels_table.csv\"), index=False)\n",
    "    curves_df.head(2000).to_csv(os.path.join(outdir, \"tables\", \"lightcurves_preview.csv\"), index=False)\n",
    "\n",
    "    # 2) Feature extraction\n",
    "    feat_df = build_feature_table(curves_df)\n",
    "    data = feat_df.merge(labels_df, on=\"star_id\", how=\"inner\")\n",
    "\n",
    "    # Define targets\n",
    "    y_cls = data[\"has_transit\"].to_numpy().astype(int)\n",
    "    groups = data[\"star_id\"].to_numpy().astype(int)\n",
    "\n",
    "    # Regression only for positive class\n",
    "    pos_mask = data[\"has_transit\"].to_numpy().astype(int) == 1\n",
    "    y_reg = data.loc[pos_mask, \"period_days\"].to_numpy().astype(float)\n",
    "\n",
    "    # Feature matrix\n",
    "    feature_cols = [\n",
    "        \"mad\", \"std\", \"p01\", \"p05\", \"p95\", \"p99\",\n",
    "        \"neg_tail\", \"dip_score\", \"acf1\", \"acf5\",\n",
    "        \"fft_peak_freq\", \"fft_peak_power\", \"fft_power_ratio\"\n",
    "    ]\n",
    "    X = data[feature_cols].to_numpy().astype(float)\n",
    "    X_pos = data.loc[pos_mask, feature_cols].to_numpy().astype(float)\n",
    "\n",
    "    # 3) Two split strategies:\n",
    "    #    A) Random split (optimistic)\n",
    "    X_tr_r, X_te_r, y_tr_r, y_te_r = train_test_split(X, y_cls, test_size=0.25, random_state=seed, stratify=y_cls)\n",
    "\n",
    "    #    B) Group split by star_id (more realistic generalisation)\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.25, random_state=seed)\n",
    "    tr_idx, te_idx = next(gss.split(X, y_cls, groups=groups))\n",
    "    X_tr_g, X_te_g = X[tr_idx], X[te_idx]\n",
    "    y_tr_g, y_te_g = y_cls[tr_idx], y_cls[te_idx]\n",
    "\n",
    "    # 4) Choose model\n",
    "    if model_kind == \"lr\":\n",
    "        base_clf = Pipeline(\n",
    "            steps=[\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\")),\n",
    "            ]\n",
    "        )\n",
    "        base_reg = Pipeline(\n",
    "            steps=[\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"reg\", Ridge(alpha=1.0)),\n",
    "            ]\n",
    "        )\n",
    "    elif model_kind == \"rf\":\n",
    "        base_clf = RandomForestClassifier(\n",
    "            n_estimators=600,\n",
    "            random_state=seed,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=3,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "        )\n",
    "        base_reg = RandomForestRegressor(\n",
    "            n_estimators=800,\n",
    "            random_state=seed,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=3,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"model_kind must be one of: 'lr', 'rf'\")\n",
    "\n",
    "    # 5) Fit + calibrate classifier on each split\n",
    "    # Use calibration to showcase reliability\n",
    "    # Note: CalibratedClassifierCV expects an unfitted base estimator\n",
    "    def fit_calibrated_classifier(X_train: np.ndarray, y_train: np.ndarray) -> CalibratedClassifierCV:\n",
    "        cal = CalibratedClassifierCV(estimator=base_clf, method=\"isotonic\", cv=3)\n",
    "        cal.fit(X_train, y_train)\n",
    "        return cal\n",
    "\n",
    "    clf_r = fit_calibrated_classifier(X_tr_r, y_tr_r)\n",
    "    clf_g = fit_calibrated_classifier(X_tr_g, y_tr_g)\n",
    "\n",
    "    proba_r = clf_r.predict_proba(X_te_r)[:, 1]\n",
    "    proba_g = clf_g.predict_proba(X_te_g)[:, 1]\n",
    "\n",
    "    metrics_cls_random = classification_metrics(y_te_r, proba_r)\n",
    "    metrics_cls_group = classification_metrics(y_te_g, proba_g)\n",
    "\n",
    "    # 6) Regression model for period estimation (positives only)\n",
    "    # We'll create regression train/test using same group split logic within positives\n",
    "    # Random split (positives)\n",
    "    Xp_tr_r, Xp_te_r, yr_tr_r, yr_te_r = train_test_split(\n",
    "        X_pos, y_reg, test_size=0.25, random_state=seed\n",
    "    )\n",
    "\n",
    "    # Group split within positives\n",
    "    pos_star_ids = data.loc[pos_mask, \"star_id\"].to_numpy().astype(int)\n",
    "    gss2 = GroupShuffleSplit(n_splits=1, test_size=0.25, random_state=seed)\n",
    "    trp_idx, tep_idx = next(gss2.split(X_pos, y_reg, groups=pos_star_ids))\n",
    "    Xp_tr_g, Xp_te_g = X_pos[trp_idx], X_pos[tep_idx]\n",
    "    yr_tr_g, yr_te_g = y_reg[trp_idx], y_reg[tep_idx]\n",
    "\n",
    "    # Fit regression models\n",
    "    reg_r = base_reg\n",
    "    reg_g = base_reg\n",
    "    reg_r.fit(Xp_tr_r, yr_tr_r)\n",
    "    reg_g.fit(Xp_tr_g, yr_tr_g)\n",
    "\n",
    "    pred_pr = reg_r.predict(Xp_te_r)\n",
    "    pred_pg = reg_g.predict(Xp_te_g)\n",
    "\n",
    "    metrics_reg_random = regression_metrics(yr_te_r, pred_pr)\n",
    "    metrics_reg_group = regression_metrics(yr_te_g, pred_pg)\n",
    "\n",
    "    # 7) OOD dataset evaluation\n",
    "    curves_ood, labels_ood = simulate_ood_dataset(rng, cfg)\n",
    "    feat_ood = build_feature_table(curves_ood)\n",
    "    data_ood = feat_ood.merge(labels_ood, on=\"star_id\", how=\"inner\")\n",
    "\n",
    "    X_ood = data_ood[feature_cols].to_numpy().astype(float)\n",
    "    y_ood = data_ood[\"has_transit\"].to_numpy().astype(int)\n",
    "    proba_ood_r = clf_r.predict_proba(X_ood)[:, 1]\n",
    "    proba_ood_g = clf_g.predict_proba(X_ood)[:, 1]\n",
    "\n",
    "    metrics_ood_from_random = classification_metrics(y_ood, proba_ood_r)\n",
    "    metrics_ood_from_group = classification_metrics(y_ood, proba_ood_g)\n",
    "\n",
    "    # 8) Save metrics JSON\n",
    "    report = {\n",
    "        \"config\": vars(cfg),\n",
    "        \"seed\": seed,\n",
    "        \"n_stars\": n_stars,\n",
    "        \"model_kind\": model_kind,\n",
    "        \"classification\": {\n",
    "            \"random_split\": metrics_cls_random,\n",
    "            \"group_split\": metrics_cls_group,\n",
    "            \"ood_from_random_model\": metrics_ood_from_random,\n",
    "            \"ood_from_group_model\": metrics_ood_from_group,\n",
    "        },\n",
    "        \"regression_period_days\": {\n",
    "            \"random_split\": metrics_reg_random,\n",
    "            \"group_split\": metrics_reg_group,\n",
    "        },\n",
    "    }\n",
    "    save_json(report, os.path.join(outdir, \"metrics_report.json\"))\n",
    "\n",
    "    # 9) Produce figures\n",
    "    figdir = os.path.join(outdir, \"figures\")\n",
    "\n",
    "    # Example light curves (4 random stars)\n",
    "    example_ids = rng.choice(labels_df[\"star_id\"].to_numpy(), size=4, replace=False)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for i, sid in enumerate(example_ids, start=1):\n",
    "        sub = curves_df[curves_df[\"star_id\"] == sid].sort_values(\"t\")\n",
    "        plt.subplot(2, 2, i)  # minimal multi-panel just for examples\n",
    "        plt.plot(sub[\"t\"], sub[\"flux\"], linewidth=1.0)\n",
    "        lab = labels_df.loc[labels_df[\"star_id\"] == sid, \"has_transit\"].iloc[0]\n",
    "        plt.title(f\"Star {sid} | transit={lab}\")\n",
    "        plt.xlabel(\"Time (days)\")\n",
    "        plt.ylabel(\"Flux\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"example_lightcurves.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # Calibration plots\n",
    "    plot_reliability(\n",
    "        y_te_r, proba_r,\n",
    "        title=\"Reliability Diagram, Random Split (Calibrated)\",\n",
    "        outpath=os.path.join(figdir, \"reliability_random_split.png\"),\n",
    "    )\n",
    "    plot_reliability(\n",
    "        y_te_g, proba_g,\n",
    "        title=\"Reliability Diagram, Group Split (Calibrated)\",\n",
    "        outpath=os.path.join(figdir, \"reliability_group_split.png\"),\n",
    "    )\n",
    "    plot_reliability(\n",
    "        y_ood, proba_ood_g,\n",
    "        title=\"Reliability Diagram, OOD Test (Group-Trained Calibrated)\",\n",
    "        outpath=os.path.join(figdir, \"reliability_ood.png\"),\n",
    "    )\n",
    "\n",
    "    # Risk–coverage curves\n",
    "    plot_risk_coverage(\n",
    "        y_te_r, proba_r,\n",
    "        title=\"Risk–Coverage, Random Split\",\n",
    "        outpath=os.path.join(figdir, \"risk_coverage_random.png\"),\n",
    "    )\n",
    "    plot_risk_coverage(\n",
    "        y_te_g, proba_g,\n",
    "        title=\"Risk–Coverage, Group Split\",\n",
    "        outpath=os.path.join(figdir, \"risk_coverage_group.png\"),\n",
    "    )\n",
    "    plot_risk_coverage(\n",
    "        y_ood, proba_ood_g,\n",
    "        title=\"Risk–Coverage, OOD (Group-Trained)\",\n",
    "        outpath=os.path.join(figdir, \"risk_coverage_ood.png\"),\n",
    "    )\n",
    "\n",
    "    # Performance summary bar chart\n",
    "    plt.figure(figsize=(8.2, 5.2))\n",
    "    names = [\"Random\", \"Group\", \"OOD(Random)\", \"OOD(Group)\"]\n",
    "    aucs = [\n",
    "        report[\"classification\"][\"random_split\"][\"roc_auc\"],\n",
    "        report[\"classification\"][\"group_split\"][\"roc_auc\"],\n",
    "        report[\"classification\"][\"ood_from_random_model\"][\"roc_auc\"],\n",
    "        report[\"classification\"][\"ood_from_group_model\"][\"roc_auc\"],\n",
    "    ]\n",
    "    plt.bar(names, aucs)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.ylabel(\"ROC-AUC\")\n",
    "    plt.title(\"Transit Detection Performance under Split Strategy and Shift\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"auc_summary.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # Regression scatter plots\n",
    "    plt.figure(figsize=(6.4, 5.2))\n",
    "    plt.scatter(yr_te_g, pred_pg, s=18)\n",
    "    lo = min(np.min(yr_te_g), np.min(pred_pg))\n",
    "    hi = max(np.max(yr_te_g), np.max(pred_pg))\n",
    "    plt.plot([lo, hi], [lo, hi])\n",
    "    plt.xlabel(\"True period (days)\")\n",
    "    plt.ylabel(\"Predicted period (days)\")\n",
    "    plt.title(\"Period Estimation, Group Split\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"period_regression_group.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 10) Save a concise results table\n",
    "    rows = []\n",
    "    for k, v in report[\"classification\"].items():\n",
    "        rows.append({\"setting\": k, **v})\n",
    "    cls_table = pd.DataFrame(rows)\n",
    "    cls_table.to_csv(os.path.join(outdir, \"tables\", \"classification_metrics.csv\"), index=False)\n",
    "\n",
    "    reg_table = pd.DataFrame(\n",
    "        [\n",
    "            {\"setting\": \"random_split\", **report[\"regression_period_days\"][\"random_split\"]},\n",
    "            {\"setting\": \"group_split\", **report[\"regression_period_days\"][\"group_split\"]},\n",
    "        ]\n",
    "    )\n",
    "    reg_table.to_csv(os.path.join(outdir, \"tables\", \"regression_metrics.csv\"), index=False)\n",
    "\n",
    "    # Print a readable summary\n",
    "    print(\"\\n=== Data Science for Astronomy PoC ===\")\n",
    "    print(f\"Outputs saved to: {os.path.abspath(outdir)}\\n\")\n",
    "    print(\"Classification (Transit detection)\")\n",
    "    print(\"  Random split:\", report[\"classification\"][\"random_split\"])\n",
    "    print(\"  Group split :\", report[\"classification\"][\"group_split\"])\n",
    "    print(\"  OOD (rand)  :\", report[\"classification\"][\"ood_from_random_model\"])\n",
    "    print(\"  OOD (group) :\", report[\"classification\"][\"ood_from_group_model\"])\n",
    "    print(\"\\nRegression (Period estimation, positives only)\")\n",
    "    print(\"  Random split:\", report[\"regression_period_days\"][\"random_split\"])\n",
    "    print(\"  Group split :\", report[\"regression_period_days\"][\"group_split\"])\n",
    "    print(\"\\nKey figures written to outputs/figures/\")\n",
    "\n",
    "\n",
    "def make_argparser() -> argparse.ArgumentParser:\n",
    "    p = argparse.ArgumentParser(description=\"UH Data Science (Physics/Astronomy/Math) Proof-of-Concept\")\n",
    "    p.add_argument(\"--outdir\", type=str, default=\"uh_poc_outputs\", help=\"Output directory for figures/tables\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "    p.add_argument(\"--n_stars\", type=int, default=300, help=\"Number of synthetic stars\")\n",
    "    p.add_argument(\"--model\", type=str, default=\"rf\", choices=[\"rf\", \"lr\"], help=\"Model type\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = make_argparser()\n",
    "    args, _ = parser.parse_known_args()  # Robust in Jupyter + terminal\n",
    "    run_experiment(outdir=args.outdir, seed=args.seed, n_stars=args.n_stars, model_kind=args.model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40ea492-1058-41c4-ba27-63fba70c55b7",
   "metadata": {},
   "source": [
    "# Inverse Problems in Physics with Reliable Data Science: Damped Harmonic Oscillator Parameter Recovery (γ, ω0) under Noise and Shift\n",
    "\n",
    "## What this script demonstrates (end-to-end, reproducible):\n",
    "    1) Physics-informed synthetic dataset generation:\n",
    "         x(t) = A * exp(-γ t) * cos(ω_d t + φ) + noise\n",
    "         ω_d = sqrt(ω0^2 - γ^2), underdamped regime\n",
    "    2) Feature engineering:\n",
    "         - time-domain stats\n",
    "         - FFT peak frequency and power ratio\n",
    "         - envelope decay estimation (log amplitude slope)\n",
    "         - autocorrelation features\n",
    "    3) Two modelling approaches:\n",
    "         - Mechanistic baseline estimators (physics-inspired)\n",
    "         - ML models: RandomForestRegressor and GaussianProcessRegressor (uncertainty intervals)\n",
    "    4) Robust evaluation:\n",
    "         - random split\n",
    "         - group split (system_id grouping) to reduce leakage bias\n",
    "    5) OOD testing:\n",
    "         - increased noise, altered sampling cadence\n",
    "    6) Uncertainty:\n",
    "         - GPR predictive mean and standard deviation\n",
    "         - coverage of 95% predictive interval\n",
    "    7) Reproducible outputs:\n",
    "         - figures, tables, metrics JSON saved to --outdir\n",
    "\n",
    "## Dependencies:\n",
    "    numpy, pandas, matplotlib, scikit-learn\n",
    "\n",
    "## Run:\n",
    "    python uh_data_science_physics_math_poc.py --outdir outputs_physics_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8afe8aa7-7684-4e83-9456-0330a1999572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 2 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 5 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 7 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 2 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 3 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 4 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Physics/Math Inverse Problem PoC ===\n",
      "Outputs saved to: /Users/petalc01/Data Science Hertfordshire/uh_physics_math_outputs\n",
      "\n",
      "Group split test:\n",
      "  Baseline mechanistic γ : {'mae': 0.11148170648767203, 'rmse': 0.17795654283805826, 'r2': 0.6344245828677941}\n",
      "  Baseline mechanistic ω0: {'mae': 0.3060038658998829, 'rmse': 0.3587402911167564, 'r2': 0.9847316711357893}\n",
      "  RF γ : {'mae': 0.03470737406859617, 'rmse': 0.055460558201923936, 'r2': 0.9644927092212835}\n",
      "  RF ω0: {'mae': 0.2926037976288964, 'rmse': 0.36551290850530754, 'r2': 0.9841497312064174}\n",
      "  GPR γ : {'mae': 0.029246244025369197, 'rmse': 0.0455891117214195, 'r2': 0.9760077308617868, 'coverage_95': 0.93, 'avg_interval_width': 0.14765038874171757}\n",
      "  GPR ω0: {'mae': 0.26431113598559564, 'rmse': 0.3197315266597091, 'r2': 0.9878716389818153, 'coverage_95': 0.98, 'avg_interval_width': 1.4933269007110834}\n",
      "\n",
      "OOD test:\n",
      "  RF γ : {'mae': 0.10577005061123643, 'rmse': 0.16041179107629702, 'r2': 0.7515129011017906}\n",
      "  RF ω0: {'mae': 0.3560132198662352, 'rmse': 0.45165402473378036, 'r2': 0.9766572476487957}\n",
      "  GPR γ : {'mae': 0.05031329170187565, 'rmse': 0.07519237649452727, 'r2': 0.9454017287177563, 'coverage_95': 0.82, 'avg_interval_width': 0.1928198539311898}\n",
      "  GPR ω0: {'mae': 0.33150720093738295, 'rmse': 0.42202143466468167, 'r2': 0.9796197591387937, 'coverage_95': 0.9175, 'avg_interval_width': 1.5093734468100082}\n",
      "\n",
      "Key figures written to outputs/figures/\n",
      "Key tables written to outputs/tables/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Silence ONLY the specific warning you reported (optional but clean).\n",
    "# Keeping this is harmless even after fixing the kernel bounds.\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*parameter k2__noise_level is close to the specified lower bound.*\",\n",
    "    category=ConvergenceWarning,\n",
    "    module=r\"sklearn\\.gaussian_process\\.kernels\",\n",
    ")\n",
    "\n",
    "# Reproducibility utilities\n",
    "\n",
    "def set_seed(seed: int = 42) -> np.random.Generator:\n",
    "    return np.random.default_rng(seed)\n",
    "\n",
    "\n",
    "def ensure_outdir(outdir: str) -> str:\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(outdir, \"figures\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(outdir, \"tables\"), exist_ok=True)\n",
    "    return outdir\n",
    "\n",
    "\n",
    "def save_json(obj: Dict, path: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "# Physics generator\n",
    "\n",
    "@dataclass\n",
    "class OscillatorConfig:\n",
    "    n_systems: int = 400\n",
    "    duration: float = 8.0        # seconds\n",
    "    min_obs: int = 250\n",
    "    max_obs: int = 420\n",
    "    noise_sigma: float = 0.03\n",
    "    cadence_min: float = 0.010   # seconds\n",
    "    cadence_max: float = 0.030   # seconds\n",
    "\n",
    "    # parameter ranges\n",
    "    A_range: Tuple[float, float] = (0.8, 2.2)\n",
    "    gamma_range: Tuple[float, float] = (0.10, 1.20)  # damping γ\n",
    "    omega0_range: Tuple[float, float] = (3.5, 14.0)  # natural ω0\n",
    "    phi_range: Tuple[float, float] = (0.0, 2*np.pi)\n",
    "\n",
    "\n",
    "def simulate_damped_oscillator(\n",
    "    rng: np.random.Generator,\n",
    "    n_obs: int,\n",
    "    duration: float,\n",
    "    cadence_range: Tuple[float, float],\n",
    "    noise_sigma: float,\n",
    "    A_range: Tuple[float, float],\n",
    "    gamma_range: Tuple[float, float],\n",
    "    omega0_range: Tuple[float, float],\n",
    "    phi_range: Tuple[float, float],\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Underdamped oscillator:\n",
    "        x(t) = A e^{-γt} cos(ω_d t + φ) + ε\n",
    "        ω_d = sqrt(ω0^2 - γ^2), enforce ω0 > γ\n",
    "    \"\"\"\n",
    "    # sample parameters, enforce underdamped regime\n",
    "    A = rng.uniform(*A_range)\n",
    "    gamma = rng.uniform(*gamma_range)\n",
    "\n",
    "    # guarantee omega0 > gamma with margin\n",
    "    omega0 = rng.uniform(max(omega0_range[0], gamma + 0.5), omega0_range[1])\n",
    "    phi = rng.uniform(*phi_range)\n",
    "\n",
    "    omega_d = math.sqrt(max(omega0**2 - gamma**2, 1e-12))\n",
    "\n",
    "    # irregular sampling\n",
    "    cadence = rng.uniform(*cadence_range)\n",
    "    t = np.arange(0.0, duration, cadence)\n",
    "    if len(t) > n_obs:\n",
    "        t = t[:n_obs]\n",
    "    else:\n",
    "        while len(t) < n_obs:\n",
    "            cadence2 = rng.uniform(*cadence_range)\n",
    "            t2 = np.arange(0.0, duration, cadence2)\n",
    "            t = np.unique(np.concatenate([t, t2]))\n",
    "            t = t[:n_obs]\n",
    "    t = np.sort(t)\n",
    "\n",
    "    # signal + noise\n",
    "    x_clean = A * np.exp(-gamma * t) * np.cos(omega_d * t + phi)\n",
    "    x = x_clean + rng.normal(0.0, noise_sigma, size=t.shape)\n",
    "\n",
    "    meta = {\n",
    "        \"A\": float(A),\n",
    "        \"gamma\": float(gamma),\n",
    "        \"omega0\": float(omega0),\n",
    "        \"omega_d\": float(omega_d),\n",
    "        \"phi\": float(phi),\n",
    "        \"noise_sigma\": float(noise_sigma),\n",
    "    }\n",
    "    return t, x, meta\n",
    "\n",
    "\n",
    "def simulate_dataset(rng: np.random.Generator, cfg: OscillatorConfig) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        series_df: long-form time series table [system_id, t, x]\n",
    "        labels_df: per-system parameters [system_id, gamma, omega0, ...]\n",
    "    \"\"\"\n",
    "    series = []\n",
    "    labels = []\n",
    "\n",
    "    for system_id in range(cfg.n_systems):\n",
    "        n_obs = int(rng.integers(cfg.min_obs, cfg.max_obs + 1))\n",
    "        t, x, meta = simulate_damped_oscillator(\n",
    "            rng=rng,\n",
    "            n_obs=n_obs,\n",
    "            duration=cfg.duration,\n",
    "            cadence_range=(cfg.cadence_min, cfg.cadence_max),\n",
    "            noise_sigma=cfg.noise_sigma * rng.uniform(0.85, 1.35),\n",
    "            A_range=cfg.A_range,\n",
    "            gamma_range=cfg.gamma_range,\n",
    "            omega0_range=cfg.omega0_range,\n",
    "            phi_range=cfg.phi_range,\n",
    "        )\n",
    "\n",
    "        series.append(pd.DataFrame({\"system_id\": system_id, \"t\": t, \"x\": x}))\n",
    "        labels.append({\"system_id\": system_id, **meta})\n",
    "\n",
    "    return pd.concat(series, ignore_index=True), pd.DataFrame(labels).sort_values(\"system_id\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def simulate_ood_dataset(rng: np.random.Generator, base_cfg: OscillatorConfig) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    OOD shift:\n",
    "        - higher noise\n",
    "        - different cadence regime\n",
    "    \"\"\"\n",
    "    cfg = OscillatorConfig(**vars(base_cfg))\n",
    "    cfg.noise_sigma *= 2.2\n",
    "    cfg.cadence_min *= 0.65\n",
    "    cfg.cadence_max *= 1.6\n",
    "    return simulate_dataset(rng, cfg)\n",
    "\n",
    "# Feature engineering (scientific)\n",
    "\n",
    "def _autocorr(x: np.ndarray, lag: int) -> float:\n",
    "    if lag <= 0 or lag >= len(x):\n",
    "        return 0.0\n",
    "    x0 = x[:-lag]\n",
    "    x1 = x[lag:]\n",
    "    denom = (np.std(x0) * np.std(x1) + 1e-12)\n",
    "    return float(np.mean((x0 - np.mean(x0)) * (x1 - np.mean(x1))) / denom)\n",
    "\n",
    "\n",
    "def extract_features(t: np.ndarray, x: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Features designed to recover oscillator parameters robustly:\n",
    "        - robust stats\n",
    "        - FFT peak frequency ~ ω_d / (2π)\n",
    "        - envelope decay proxy ~ γ\n",
    "        - autocorrelation at small lags\n",
    "    \"\"\"\n",
    "    x = x.astype(np.float64)\n",
    "    median = float(np.median(x))\n",
    "    mad = float(np.median(np.abs(x - median))) + 1e-12\n",
    "    y = x - median\n",
    "\n",
    "    std = float(np.std(y))\n",
    "    p05 = float(np.quantile(y, 0.05))\n",
    "    p95 = float(np.quantile(y, 0.95))\n",
    "    iqr = float(p95 - p05)\n",
    "\n",
    "    # Uniform grid interpolation for FFT\n",
    "    n_grid = 512\n",
    "    t0, t1 = float(np.min(t)), float(np.max(t))\n",
    "    if (t1 - t0) < 1e-6:\n",
    "        return {\n",
    "            \"mad\": mad, \"std\": std, \"iqr\": iqr,\n",
    "            \"acf1\": 0.0, \"acf5\": 0.0, \"acf15\": 0.0,\n",
    "            \"fft_peak_hz\": 0.0, \"fft_power_ratio\": 0.0,\n",
    "            \"env_decay\": 0.0, \"env_r2\": 0.0,\n",
    "        }\n",
    "\n",
    "    grid_t = np.linspace(t0, t1, n_grid)\n",
    "    grid_y = np.interp(grid_t, t, y)\n",
    "    grid_y = grid_y - np.mean(grid_y)\n",
    "\n",
    "    # Autocorrelation features\n",
    "    acf1 = _autocorr(grid_y, 1)\n",
    "    acf5 = _autocorr(grid_y, 5)\n",
    "    acf15 = _autocorr(grid_y, 15)\n",
    "\n",
    "    # FFT: peak frequency in Hz (cycles/sec)\n",
    "    fft = np.fft.rfft(grid_y)\n",
    "    power = np.abs(fft) ** 2\n",
    "    freqs = np.fft.rfftfreq(n_grid, d=(grid_t[1] - grid_t[0]))\n",
    "\n",
    "    if len(power) > 1:\n",
    "        power_no_dc = power.copy()\n",
    "        power_no_dc[0] = 0.0\n",
    "        idx = int(np.argmax(power_no_dc))\n",
    "        peak_hz = float(freqs[idx])\n",
    "        total_power = float(np.sum(power_no_dc) + 1e-12)\n",
    "        power_ratio = float(power_no_dc[idx] / total_power)\n",
    "    else:\n",
    "        peak_hz, power_ratio = 0.0, 0.0\n",
    "\n",
    "    # Envelope decay estimate:\n",
    "    # compute absolute signal, take log, fit slope vs time using simple linear regression formula\n",
    "    # This approximates γ since |A e^{-γt} cos(...)| envelope ~ e^{-γt}.\n",
    "    abs_y = np.abs(np.interp(grid_t, t, (x - np.median(x))))\n",
    "    eps = 1e-8\n",
    "    log_env = np.log(abs_y + eps)\n",
    "\n",
    "    # Linear regression slope for log_env ~ a + b t, b ~ -γ\n",
    "    tt = grid_t.reshape(-1)\n",
    "    yy = log_env.reshape(-1)\n",
    "\n",
    "    # Focus on upper envelope by taking quantile across sliding windows\n",
    "    # simple approach: downsample by windows, take 80th percentile as envelope proxy\n",
    "    w = 16\n",
    "    env_t = []\n",
    "    env_y = []\n",
    "    for i in range(0, len(tt) - w + 1, w):\n",
    "        window = yy[i:i+w]\n",
    "        env_t.append(np.mean(tt[i:i+w]))\n",
    "        env_y.append(np.quantile(window, 0.80))\n",
    "    env_t = np.array(env_t)\n",
    "    env_y = np.array(env_y)\n",
    "\n",
    "    # Fit env_y ~ a + b env_t\n",
    "    t_mean = float(np.mean(env_t))\n",
    "    y_mean = float(np.mean(env_y))\n",
    "    denom = float(np.sum((env_t - t_mean) ** 2) + 1e-12)\n",
    "    slope = float(np.sum((env_t - t_mean) * (env_y - y_mean)) / denom)\n",
    "    intercept = y_mean - slope * t_mean\n",
    "\n",
    "    # R2 for envelope fit\n",
    "    y_hat = intercept + slope * env_t\n",
    "    ss_res = float(np.sum((env_y - y_hat) ** 2))\n",
    "    ss_tot = float(np.sum((env_y - y_mean) ** 2) + 1e-12)\n",
    "    r2_env = float(1.0 - ss_res / ss_tot)\n",
    "\n",
    "    env_decay = float(-slope)  # estimate of gamma, roughly\n",
    "\n",
    "    return {\n",
    "        \"mad\": mad,\n",
    "        \"std\": std,\n",
    "        \"iqr\": iqr,\n",
    "        \"acf1\": acf1,\n",
    "        \"acf5\": acf5,\n",
    "        \"acf15\": acf15,\n",
    "        \"fft_peak_hz\": peak_hz,\n",
    "        \"fft_power_ratio\": power_ratio,\n",
    "        \"env_decay\": env_decay,\n",
    "        \"env_r2\": r2_env,\n",
    "    }\n",
    "\n",
    "\n",
    "def build_feature_table(series_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    feats = []\n",
    "    for system_id, sub in series_df.groupby(\"system_id\", sort=True):\n",
    "        t = sub[\"t\"].to_numpy()\n",
    "        x = sub[\"x\"].to_numpy()\n",
    "        f = extract_features(t, x)\n",
    "        f[\"system_id\"] = int(system_id)\n",
    "        feats.append(f)\n",
    "    return pd.DataFrame(feats).sort_values(\"system_id\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Baseline mechanistic estimators\n",
    "\n",
    "def baseline_estimate_gamma_omega0(feat_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Mechanistic baseline:\n",
    "        omega_d ≈ 2π * fft_peak_hz\n",
    "        gamma   ≈ env_decay (from envelope fit)\n",
    "\n",
    "    Then:\n",
    "        omega0 ≈ sqrt(omega_d^2 + gamma^2)\n",
    "    \"\"\"\n",
    "    omega_d = 2.0 * np.pi * feat_df[\"fft_peak_hz\"].to_numpy().astype(float)\n",
    "    gamma_hat = feat_df[\"env_decay\"].to_numpy().astype(float)\n",
    "\n",
    "    # stabilise\n",
    "    gamma_hat = np.clip(gamma_hat, 0.0, 5.0)\n",
    "    omega0_hat = np.sqrt(np.maximum(omega_d**2 + gamma_hat**2, 1e-12))\n",
    "    return gamma_hat, omega0_hat\n",
    "\n",
    "\n",
    "# Metrics + uncertainty helpers\n",
    "\n",
    "def reg_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return {\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"rmse\": float(rmse),\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "\n",
    "def interval_coverage(y_true: np.ndarray, mean: np.ndarray, std: np.ndarray, z: float = 1.96) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Coverage of mean ± z*std interval.\n",
    "    \"\"\"\n",
    "    lo = mean - z * std\n",
    "    hi = mean + z * std\n",
    "    covered = (y_true >= lo) & (y_true <= hi)\n",
    "    return {\n",
    "        \"coverage_95\": float(np.mean(covered)),\n",
    "        \"avg_interval_width\": float(np.mean(hi - lo)),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_scatter(y_true: np.ndarray, y_pred: np.ndarray, title: str, outpath: str) -> None:\n",
    "    plt.figure(figsize=(6.4, 5.2))\n",
    "    plt.scatter(y_true, y_pred, s=18)\n",
    "    lo = float(min(np.min(y_true), np.min(y_pred)))\n",
    "    hi = float(max(np.max(y_true), np.max(y_pred)))\n",
    "    plt.plot([lo, hi], [lo, hi])\n",
    "    plt.xlabel(\"True\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_interval_example(t: np.ndarray, x: np.ndarray, title: str, outpath: str) -> None:\n",
    "    plt.figure(figsize=(6.4, 4.6))\n",
    "    plt.plot(t, x, linewidth=1.0)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Displacement x(t)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Main experiment\n",
    "\n",
    "def run_experiment(outdir: str, seed: int = 42, n_systems: int = 400) -> None:\n",
    "    rng = set_seed(seed)\n",
    "    ensure_outdir(outdir)\n",
    "\n",
    "    # 1) Generate dataset\n",
    "    cfg = OscillatorConfig(n_systems=n_systems)\n",
    "    series_df, labels_df = simulate_dataset(rng, cfg)\n",
    "\n",
    "    # Save data previews\n",
    "    labels_df.to_csv(os.path.join(outdir, \"tables\", \"labels_table.csv\"), index=False)\n",
    "    series_df.head(2500).to_csv(os.path.join(outdir, \"tables\", \"timeseries_preview.csv\"), index=False)\n",
    "\n",
    "    # 2) Feature extraction\n",
    "    feat_df = build_feature_table(series_df)\n",
    "    data = feat_df.merge(labels_df, on=\"system_id\", how=\"inner\")\n",
    "\n",
    "    feature_cols = [\"mad\", \"std\", \"iqr\", \"acf1\", \"acf5\", \"acf15\", \"fft_peak_hz\", \"fft_power_ratio\", \"env_decay\", \"env_r2\"]\n",
    "    X = data[feature_cols].to_numpy().astype(float)\n",
    "    groups = data[\"system_id\"].to_numpy().astype(int)\n",
    "\n",
    "    y_gamma = data[\"gamma\"].to_numpy().astype(float)\n",
    "    y_omega0 = data[\"omega0\"].to_numpy().astype(float)\n",
    "\n",
    "    # 3) Two split strategies\n",
    "    # Random split\n",
    "    X_tr_r, X_te_r, yg_tr_r, yg_te_r, y0_tr_r, y0_te_r = train_test_split(\n",
    "        X, y_gamma, y_omega0, test_size=0.25, random_state=seed\n",
    "    )\n",
    "\n",
    "    # Group split\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.25, random_state=seed)\n",
    "    tr_idx, te_idx = next(gss.split(X, y_gamma, groups=groups))\n",
    "    X_tr_g, X_te_g = X[tr_idx], X[te_idx]\n",
    "    yg_tr_g, yg_te_g = y_gamma[tr_idx], y_gamma[te_idx]\n",
    "    y0_tr_g, y0_te_g = y_omega0[tr_idx], y_omega0[te_idx]\n",
    "\n",
    "    # 4) Baseline mechanistic estimates (no ML)\n",
    "    gamma_base, omega0_base = baseline_estimate_gamma_omega0(data.loc[te_idx].reset_index(drop=True))\n",
    "\n",
    "    base_gamma_metrics = reg_metrics(yg_te_g, gamma_base)\n",
    "    base_omega0_metrics = reg_metrics(y0_te_g, omega0_base)\n",
    "\n",
    "    # 5) ML models\n",
    "    # RandomForest (strong non-linear baseline)\n",
    "    rf_gamma = RandomForestRegressor(n_estimators=700, random_state=seed, min_samples_leaf=3)\n",
    "    rf_omega0 = RandomForestRegressor(n_estimators=700, random_state=seed, min_samples_leaf=3)\n",
    "\n",
    "    rf_gamma.fit(X_tr_g, yg_tr_g)\n",
    "    rf_omega0.fit(X_tr_g, y0_tr_g)\n",
    "\n",
    "    pred_rf_gamma = rf_gamma.predict(X_te_g)\n",
    "    pred_rf_omega0 = rf_omega0.predict(X_te_g)\n",
    "\n",
    "    rf_gamma_metrics = reg_metrics(yg_te_g, pred_rf_gamma)\n",
    "    rf_omega0_metrics = reg_metrics(y0_te_g, pred_rf_omega0)\n",
    "\n",
    "    # Gaussian Process Regression (uncertainty-aware)\n",
    "    # Kernel: constant * RBF + white noise\n",
    "    kernel = ConstantKernel(1.0, (1e-2, 1e2)) * RBF(length_scale=np.ones(X.shape[1]), length_scale_bounds=(1e-2, 1e2)) \\\n",
    "             + WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-6, 1e1))\n",
    "\n",
    "    gpr_gamma = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True, random_state=seed)\n",
    "    gpr_omega0 = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True, random_state=seed)\n",
    "\n",
    "    gpr_gamma.fit(X_tr_g, yg_tr_g)\n",
    "    gpr_omega0.fit(X_tr_g, y0_tr_g)\n",
    "\n",
    "    mean_g_gamma, std_g_gamma = gpr_gamma.predict(X_te_g, return_std=True)\n",
    "    mean_g_omega0, std_g_omega0 = gpr_omega0.predict(X_te_g, return_std=True)\n",
    "\n",
    "    gpr_gamma_metrics = reg_metrics(yg_te_g, mean_g_gamma)\n",
    "    gpr_omega0_metrics = reg_metrics(y0_te_g, mean_g_omega0)\n",
    "\n",
    "    gpr_gamma_unc = interval_coverage(yg_te_g, mean_g_gamma, std_g_gamma)\n",
    "    gpr_omega0_unc = interval_coverage(y0_te_g, mean_g_omega0, std_g_omega0)\n",
    "\n",
    "    # 6) OOD test (noise + cadence shift)\n",
    "    series_ood, labels_ood = simulate_ood_dataset(rng, cfg)\n",
    "    feat_ood = build_feature_table(series_ood)\n",
    "    data_ood = feat_ood.merge(labels_ood, on=\"system_id\", how=\"inner\")\n",
    "\n",
    "    X_ood = data_ood[feature_cols].to_numpy().astype(float)\n",
    "    yg_ood = data_ood[\"gamma\"].to_numpy().astype(float)\n",
    "    y0_ood = data_ood[\"omega0\"].to_numpy().astype(float)\n",
    "\n",
    "    # Evaluate RF and GPR on OOD\n",
    "    pred_rf_gamma_ood = rf_gamma.predict(X_ood)\n",
    "    pred_rf_omega0_ood = rf_omega0.predict(X_ood)\n",
    "\n",
    "    mean_g_gamma_ood, std_g_gamma_ood = gpr_gamma.predict(X_ood, return_std=True)\n",
    "    mean_g_omega0_ood, std_g_omega0_ood = gpr_omega0.predict(X_ood, return_std=True)\n",
    "\n",
    "    rf_gamma_ood_metrics = reg_metrics(yg_ood, pred_rf_gamma_ood)\n",
    "    rf_omega0_ood_metrics = reg_metrics(y0_ood, pred_rf_omega0_ood)\n",
    "\n",
    "    gpr_gamma_ood_metrics = reg_metrics(yg_ood, mean_g_gamma_ood)\n",
    "    gpr_omega0_ood_metrics = reg_metrics(y0_ood, mean_g_omega0_ood)\n",
    "\n",
    "    gpr_gamma_ood_unc = interval_coverage(yg_ood, mean_g_gamma_ood, std_g_gamma_ood)\n",
    "    gpr_omega0_ood_unc = interval_coverage(y0_ood, mean_g_omega0_ood, std_g_omega0_ood)\n",
    "\n",
    "    # 7) Save metrics report\n",
    "    report = {\n",
    "        \"config\": vars(cfg),\n",
    "        \"seed\": seed,\n",
    "        \"n_systems\": n_systems,\n",
    "        \"group_split_results\": {\n",
    "            \"baseline_mechanistic\": {\n",
    "                \"gamma\": base_gamma_metrics,\n",
    "                \"omega0\": base_omega0_metrics,\n",
    "            },\n",
    "            \"random_forest\": {\n",
    "                \"gamma\": rf_gamma_metrics,\n",
    "                \"omega0\": rf_omega0_metrics,\n",
    "            },\n",
    "            \"gaussian_process\": {\n",
    "                \"gamma\": {**gpr_gamma_metrics, **gpr_gamma_unc},\n",
    "                \"omega0\": {**gpr_omega0_metrics, **gpr_omega0_unc},\n",
    "            },\n",
    "        },\n",
    "        \"ood_results\": {\n",
    "            \"random_forest\": {\n",
    "                \"gamma\": rf_gamma_ood_metrics,\n",
    "                \"omega0\": rf_omega0_ood_metrics,\n",
    "            },\n",
    "            \"gaussian_process\": {\n",
    "                \"gamma\": {**gpr_gamma_ood_metrics, **gpr_gamma_ood_unc},\n",
    "                \"omega0\": {**gpr_omega0_ood_metrics, **gpr_omega0_ood_unc},\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    save_json(report, os.path.join(outdir, \"metrics_report.json\"))\n",
    "\n",
    "    # 8) Produce figures\n",
    "    figdir = os.path.join(outdir, \"figures\")\n",
    "\n",
    "    # Example time series plots (train domain + OOD)\n",
    "    example_ids = rng.choice(labels_df[\"system_id\"].to_numpy(), size=2, replace=False)\n",
    "    for sid in example_ids:\n",
    "        sub = series_df[series_df[\"system_id\"] == sid].sort_values(\"t\")\n",
    "        plot_interval_example(\n",
    "            sub[\"t\"].to_numpy(),\n",
    "            sub[\"x\"].to_numpy(),\n",
    "            title=f\"Damped Oscillator Example (system {sid})\",\n",
    "            outpath=os.path.join(figdir, f\"example_series_{sid}.png\"),\n",
    "        )\n",
    "\n",
    "    example_ids_ood = rng.choice(labels_ood[\"system_id\"].to_numpy(), size=2, replace=False)\n",
    "    for sid in example_ids_ood:\n",
    "        sub = series_ood[series_ood[\"system_id\"] == sid].sort_values(\"t\")\n",
    "        plot_interval_example(\n",
    "            sub[\"t\"].to_numpy(),\n",
    "            sub[\"x\"].to_numpy(),\n",
    "            title=f\"OOD Example (system {sid})\",\n",
    "            outpath=os.path.join(figdir, f\"example_series_ood_{sid}.png\"),\n",
    "        )\n",
    "\n",
    "    # Scatter plots (group test)\n",
    "    plot_scatter(\n",
    "        yg_te_g, gamma_base,\n",
    "        title=\"Baseline Mechanistic: γ (Group Split Test)\",\n",
    "        outpath=os.path.join(figdir, \"scatter_baseline_gamma.png\"),\n",
    "    )\n",
    "    plot_scatter(\n",
    "        y0_te_g, omega0_base,\n",
    "        title=\"Baseline Mechanistic: ω0 (Group Split Test)\",\n",
    "        outpath=os.path.join(figdir, \"scatter_baseline_omega0.png\"),\n",
    "    )\n",
    "    plot_scatter(\n",
    "        yg_te_g, pred_rf_gamma,\n",
    "        title=\"Random Forest: γ (Group Split Test)\",\n",
    "        outpath=os.path.join(figdir, \"scatter_rf_gamma.png\"),\n",
    "    )\n",
    "    plot_scatter(\n",
    "        y0_te_g, pred_rf_omega0,\n",
    "        title=\"Random Forest: ω0 (Group Split Test)\",\n",
    "        outpath=os.path.join(figdir, \"scatter_rf_omega0.png\"),\n",
    "    )\n",
    "    plot_scatter(\n",
    "        yg_te_g, mean_g_gamma,\n",
    "        title=\"Gaussian Process: γ mean (Group Split Test)\",\n",
    "        outpath=os.path.join(figdir, \"scatter_gpr_gamma.png\"),\n",
    "    )\n",
    "    plot_scatter(\n",
    "        y0_te_g, mean_g_omega0,\n",
    "        title=\"Gaussian Process: ω0 mean (Group Split Test)\",\n",
    "        outpath=os.path.join(figdir, \"scatter_gpr_omega0.png\"),\n",
    "    )\n",
    "\n",
    "    # Uncertainty diagnostics: predicted std vs absolute error (GPR)\n",
    "    abs_err_gamma = np.abs(yg_te_g - mean_g_gamma)\n",
    "    abs_err_omega0 = np.abs(y0_te_g - mean_g_omega0)\n",
    "\n",
    "    plt.figure(figsize=(6.4, 5.2))\n",
    "    plt.scatter(std_g_gamma, abs_err_gamma, s=18)\n",
    "    plt.xlabel(\"Predicted std (γ)\")\n",
    "    plt.ylabel(\"|Error|\")\n",
    "    plt.title(\"Uncertainty usefulness: GPR std vs |error| (γ)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"uncertainty_gamma_std_vs_error.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6.4, 5.2))\n",
    "    plt.scatter(std_g_omega0, abs_err_omega0, s=18)\n",
    "    plt.xlabel(\"Predicted std (ω0)\")\n",
    "    plt.ylabel(\"|Error|\")\n",
    "    plt.title(\"Uncertainty usefulness: GPR std vs |error| (ω0)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"uncertainty_omega0_std_vs_error.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 9) Save compact tables\n",
    "    # Group split metrics table\n",
    "    rows = []\n",
    "    rows.append({\"model\": \"baseline_mechanistic\", \"target\": \"gamma\", **base_gamma_metrics})\n",
    "    rows.append({\"model\": \"baseline_mechanistic\", \"target\": \"omega0\", **base_omega0_metrics})\n",
    "    rows.append({\"model\": \"random_forest\", \"target\": \"gamma\", **rf_gamma_metrics})\n",
    "    rows.append({\"model\": \"random_forest\", \"target\": \"omega0\", **rf_omega0_metrics})\n",
    "    rows.append({\"model\": \"gaussian_process\", \"target\": \"gamma\", **{**gpr_gamma_metrics, **gpr_gamma_unc}})\n",
    "    rows.append({\"model\": \"gaussian_process\", \"target\": \"omega0\", **{**gpr_omega0_metrics, **gpr_omega0_unc}})\n",
    "\n",
    "    group_table = pd.DataFrame(rows)\n",
    "    group_table.to_csv(os.path.join(outdir, \"tables\", \"group_split_metrics.csv\"), index=False)\n",
    "\n",
    "    # OOD metrics table\n",
    "    rows_ood = []\n",
    "    rows_ood.append({\"model\": \"random_forest\", \"target\": \"gamma\", **rf_gamma_ood_metrics})\n",
    "    rows_ood.append({\"model\": \"random_forest\", \"target\": \"omega0\", **rf_omega0_ood_metrics})\n",
    "    rows_ood.append({\"model\": \"gaussian_process\", \"target\": \"gamma\", **{**gpr_gamma_ood_metrics, **gpr_gamma_ood_unc}})\n",
    "    rows_ood.append({\"model\": \"gaussian_process\", \"target\": \"omega0\", **{**gpr_omega0_ood_metrics, **gpr_omega0_ood_unc}})\n",
    "\n",
    "    ood_table = pd.DataFrame(rows_ood)\n",
    "    ood_table.to_csv(os.path.join(outdir, \"tables\", \"ood_metrics.csv\"), index=False)\n",
    "\n",
    "    # 10) Print summary\n",
    "    print(\"\\n=== Physics/Math Inverse Problem PoC ===\")\n",
    "    print(f\"Outputs saved to: {os.path.abspath(outdir)}\\n\")\n",
    "    print(\"Group split test:\")\n",
    "    print(\"  Baseline mechanistic γ :\", base_gamma_metrics)\n",
    "    print(\"  Baseline mechanistic ω0:\", base_omega0_metrics)\n",
    "    print(\"  RF γ :\", rf_gamma_metrics)\n",
    "    print(\"  RF ω0:\", rf_omega0_metrics)\n",
    "    print(\"  GPR γ :\", {**gpr_gamma_metrics, **gpr_gamma_unc})\n",
    "    print(\"  GPR ω0:\", {**gpr_omega0_metrics, **gpr_omega0_unc})\n",
    "    print(\"\\nOOD test:\")\n",
    "    print(\"  RF γ :\", rf_gamma_ood_metrics)\n",
    "    print(\"  RF ω0:\", rf_omega0_ood_metrics)\n",
    "    print(\"  GPR γ :\", {**gpr_gamma_ood_metrics, **gpr_gamma_ood_unc})\n",
    "    print(\"  GPR ω0:\", {**gpr_omega0_ood_metrics, **gpr_omega0_ood_unc})\n",
    "    print(\"\\nKey figures written to outputs/figures/\")\n",
    "    print(\"Key tables written to outputs/tables/\\n\")\n",
    "\n",
    "\n",
    "def make_argparser() -> argparse.ArgumentParser:\n",
    "    p = argparse.ArgumentParser(description=\"UH Data Science (Physics/Math) Inverse Problem Proof-of-Concept\")\n",
    "    p.add_argument(\"--outdir\", type=str, default=\"uh_physics_math_outputs\", help=\"Output directory\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "    p.add_argument(\"--n_systems\", type=int, default=400, help=\"Number of simulated oscillators\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = make_argparser()\n",
    "    args, _ = parser.parse_known_args()  # Robust in Jupyter + terminal\n",
    "    run_experiment(outdir=args.outdir, seed=args.seed, n_systems=args.n_systems)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb2432-c1ab-4d8a-9cd3-5109630684d8",
   "metadata": {},
   "source": [
    "# Gaussian Process Bayesian Optimisation for an Expensive Physics-Style Simulator\n",
    "\n",
    "## This script demonstrates:\n",
    "    1) A synthetic but physics-inspired expensive black-box objective:\n",
    "         - Multi-modal landscape\n",
    "         - Measurement noise\n",
    "         - \"Experiment cost\" penalty\n",
    "         - Feasible region constraints (like stability limits)\n",
    "    2) Bayesian optimisation loop:\n",
    "         - Gaussian Process surrogate (with uncertainty)\n",
    "         - Acquisition functions: Expected Improvement (EI), UCB\n",
    "         - Active learning: sequential design under limited evaluation budget\n",
    "    3) Scientific reporting:\n",
    "         - convergence curves\n",
    "         - surrogate predictions + uncertainty\n",
    "         - acquisition maps (2D)\n",
    "         - tables + JSON metrics\n",
    "\n",
    "## Important fixes included:\n",
    "    Works in Jupyter notebooks (ignores ipykernel \"-f\" argument)\n",
    "    Compatible with NumPy 2.x (no np.erf, uses math.erf vectorised)\n",
    "    Numerical stability for EI (sigma floor)\n",
    "\n",
    "## Dependencies:\n",
    "    numpy, pandas, matplotlib, scikit-learn\n",
    "\n",
    "## Run:\n",
    "    python uh_bayesopt_physics_poc.py --outdir uh_bo_outputs --acq ei --budget 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "051edb80-43b1-4b18-965d-3713e8f9dd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GP Bayesian Optimisation PoC ===\n",
      "Outputs saved to: /Users/petalc01/Data Science Hertfordshire/uh_bo_outputs\n",
      "\n",
      "Best found design:\n",
      "  x1 = 0.5748359092858863\n",
      "  x2 = -0.11639897192768656\n",
      "  best y = 2.119765312067755\n",
      "\n",
      "Random search best y (same budget): 1.6852151086269096\n",
      "Efficiency gap (BO - random): 0.4345502034408453\n",
      "\n",
      "OOD shift check at best point (noiseless):\n",
      "  in-domain score : 1.9416040241658161\n",
      "  shifted score   : 1.343030715628198\n",
      "  drop            : 0.5985733085376181\n",
      "\n",
      "Key figures written to outputs/figures/\n",
      "Key tables written to outputs/tables/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Silence ONLY the specific warning you reported (optional but clean).\n",
    "# Keeping this is harmless even after fixing the kernel bounds.\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*parameter k2__noise_level is close to the specified lower bound.*\",\n",
    "    category=ConvergenceWarning,\n",
    "    module=r\"sklearn\\.gaussian_process\\.kernels\",\n",
    ")\n",
    "\n",
    "# Repro utilities\n",
    "\n",
    "def set_seed(seed: int = 42) -> np.random.Generator:\n",
    "    return np.random.default_rng(seed)\n",
    "\n",
    "\n",
    "def ensure_outdir(outdir: str) -> str:\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(outdir, \"figures\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(outdir, \"tables\"), exist_ok=True)\n",
    "    return outdir\n",
    "\n",
    "\n",
    "def save_json(obj: Dict, path: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "\n",
    "# Physics-inspired simulator\n",
    "\n",
    "@dataclass\n",
    "class SimSettings:\n",
    "    # domain bounds for two design parameters (Physics/Astro/Math style tuning knobs)\n",
    "    # x1: e.g. driving amplitude\n",
    "    # x2: e.g. coupling strength\n",
    "    x1_bounds: Tuple[float, float] = (-2.0, 2.0)\n",
    "    x2_bounds: Tuple[float, float] = (-2.0, 2.0)\n",
    "\n",
    "    # noise level representing stochastic experiment measurement noise\n",
    "    noise_sigma: float = 0.08\n",
    "\n",
    "    # cost penalty strength\n",
    "    cost_lambda: float = 0.15\n",
    "\n",
    "    # feasibility (stability) constraint severity\n",
    "    constraint_strength: float = 8.0\n",
    "\n",
    "\n",
    "def simulator_objective(\n",
    "    rng: np.random.Generator,\n",
    "    X: np.ndarray,\n",
    "    cfg: SimSettings,\n",
    "    noisy: bool = True,\n",
    "    shift: float = 0.0,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Expensive black-box objective:\n",
    "        score = \"physics performance\" - cost_lambda * \"experiment cost\" - constraint penalty + noise\n",
    "\n",
    "    Physics performance:\n",
    "        mixture of oscillatory + smooth peaks (multi-modal)\n",
    "    Cost:\n",
    "        encourages cheaper experiments (like resource usage or time)\n",
    "    Constraint:\n",
    "        feasible region in a curved manifold (simulating stability)\n",
    "    shift:\n",
    "        adds a distribution shift to simulate lab drift or regime change\n",
    "    \"\"\"\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "\n",
    "    # 1) \"Physics performance\" term (multi-modal, smooth + oscillatory)\n",
    "    # Think: resonance + interference patterns\n",
    "    perf = (\n",
    "        1.7 * np.sin(2.6 * (x1 + shift)) * np.cos(2.2 * (x2 - 0.3 * shift))\n",
    "        + 0.9 * np.exp(-((x1 - 0.8 + 0.2 * shift) ** 2 + (x2 + 0.6) ** 2) / 0.35)\n",
    "        + 1.1 * np.exp(-((x1 + 1.1) ** 2 + (x2 - 0.9 + 0.1 * shift) ** 2) / 0.25)\n",
    "        - 0.15 * (x1**2 + x2**2)\n",
    "    )\n",
    "\n",
    "    # 2) Cost term (penalise \"expensive\" settings)\n",
    "    # Example: high |x1| increases power usage, high x2 increases complexity\n",
    "    cost = 0.6 * (np.abs(x1) ** 1.3) + 0.4 * (np.abs(x2) ** 1.6)\n",
    "\n",
    "    # 3) Feasibility constraint: stable if inside a curved boundary\n",
    "    # Example constraint: x2 <= 1.2 - 0.5*x1^2 (curved stability envelope)\n",
    "    stability_limit = 1.2 - 0.5 * (x1**2)\n",
    "    violation = np.maximum(0.0, x2 - stability_limit)\n",
    "    penalty = cfg.constraint_strength * (violation ** 2)\n",
    "\n",
    "    score_clean = perf - cfg.cost_lambda * cost - penalty\n",
    "\n",
    "    # Noisy measurement\n",
    "    if noisy:\n",
    "        eps = rng.normal(0.0, cfg.noise_sigma, size=score_clean.shape)\n",
    "        score = score_clean + eps\n",
    "    else:\n",
    "        score = score_clean\n",
    "\n",
    "    return score.astype(float), cost.astype(float)\n",
    "\n",
    "\n",
    "def latin_hypercube_2d(\n",
    "    rng: np.random.Generator,\n",
    "    n: int,\n",
    "    bounds: Tuple[Tuple[float, float], Tuple[float, float]],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simple Latin Hypercube Sampling in 2D to seed BO.\n",
    "    \"\"\"\n",
    "    (a1, b1), (a2, b2) = bounds\n",
    "    u1 = (np.arange(n) + rng.random(n)) / n\n",
    "    u2 = (np.arange(n) + rng.random(n)) / n\n",
    "    rng.shuffle(u1)\n",
    "    rng.shuffle(u2)\n",
    "    x1 = a1 + (b1 - a1) * u1\n",
    "    x2 = a2 + (b2 - a2) * u2\n",
    "    return np.vstack([x1, x2]).T\n",
    "\n",
    "\n",
    "# Gaussian Process surrogate\n",
    "\n",
    "def make_gp(seed: int = 42) -> GaussianProcessRegressor:\n",
    "    kernel = (\n",
    "        ConstantKernel(1.0, (1e-2, 1e2))\n",
    "        * RBF(length_scale=np.ones(2), length_scale_bounds=(1e-2, 1e2))\n",
    "        + WhiteKernel(noise_level=1e-2, noise_level_bounds=(1e-6, 1e1))\n",
    "    )\n",
    "    gp = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        normalize_y=True,\n",
    "        random_state=seed,\n",
    "        n_restarts_optimizer=4,\n",
    "    )\n",
    "    return gp\n",
    "\n",
    "\n",
    "# Acquisition functions\n",
    "\n",
    "def normal_pdf(z: np.ndarray) -> np.ndarray:\n",
    "    return (1.0 / np.sqrt(2.0 * np.pi)) * np.exp(-0.5 * z**2)\n",
    "\n",
    "\n",
    "def normal_cdf(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    CDF of standard normal.\n",
    "    NumPy 2.x removed np.erf, so we use math.erf and vectorise it.\n",
    "    \"\"\"\n",
    "    erf_vec = np.vectorize(math.erf)\n",
    "    return 0.5 * (1.0 + erf_vec(z / np.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def expected_improvement(mu: np.ndarray, sigma: np.ndarray, best_y: float, xi: float = 0.01) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    EI for maximisation.\n",
    "    \"\"\"\n",
    "    sigma = np.maximum(sigma, 1e-12)  # numerical stability\n",
    "    imp = mu - best_y - xi\n",
    "    z = imp / sigma\n",
    "    ei = imp * normal_cdf(z) + sigma * normal_pdf(z)\n",
    "    return np.maximum(ei, 0.0)\n",
    "\n",
    "\n",
    "def upper_confidence_bound(mu: np.ndarray, sigma: np.ndarray, beta: float = 2.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    UCB for maximisation.\n",
    "    \"\"\"\n",
    "    return mu + beta * sigma\n",
    "\n",
    "\n",
    "# BO loop\n",
    "\n",
    "def propose_next_point(\n",
    "    rng: np.random.Generator,\n",
    "    gp: GaussianProcessRegressor,\n",
    "    bounds: Tuple[Tuple[float, float], Tuple[float, float]],\n",
    "    acq: str,\n",
    "    y_best: float,\n",
    "    n_candidates: int = 6000,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Use random candidate search over acquisition function for simplicity and clarity.\n",
    "    \"\"\"\n",
    "    (a1, b1), (a2, b2) = bounds\n",
    "    Xcand = np.column_stack([\n",
    "        rng.uniform(a1, b1, size=n_candidates),\n",
    "        rng.uniform(a2, b2, size=n_candidates),\n",
    "    ])\n",
    "\n",
    "    mu, std = gp.predict(Xcand, return_std=True)\n",
    "\n",
    "    if acq.lower() == \"ei\":\n",
    "        values = expected_improvement(mu, std, best_y=y_best, xi=0.01)\n",
    "    elif acq.lower() == \"ucb\":\n",
    "        values = upper_confidence_bound(mu, std, beta=2.2)\n",
    "    else:\n",
    "        raise ValueError(\"acq must be one of: ei, ucb\")\n",
    "\n",
    "    idx = int(np.argmax(values))\n",
    "    return Xcand[idx:idx + 1, :]\n",
    "\n",
    "\n",
    "def run_bayesopt(\n",
    "    outdir: str,\n",
    "    seed: int = 42,\n",
    "    budget: int = 60,\n",
    "    init_points: int = 12,\n",
    "    acq: str = \"ei\",\n",
    "    n_candidates: int = 6000,\n",
    "    shift_test: bool = True,\n",
    ") -> None:\n",
    "    rng = set_seed(seed)\n",
    "    ensure_outdir(outdir)\n",
    "\n",
    "    cfg = SimSettings()\n",
    "    bounds = (cfg.x1_bounds, cfg.x2_bounds)\n",
    "\n",
    "    # 1) Initialise design with LHS\n",
    "    X = latin_hypercube_2d(rng, init_points, bounds)\n",
    "    y, cost = simulator_objective(rng, X, cfg, noisy=True, shift=0.0)\n",
    "\n",
    "    trace_rows = []\n",
    "    for i in range(init_points):\n",
    "        trace_rows.append({\n",
    "            \"iter\": i,\n",
    "            \"x1\": float(X[i, 0]),\n",
    "            \"x2\": float(X[i, 1]),\n",
    "            \"y\": float(y[i]),\n",
    "            \"cost\": float(cost[i]),\n",
    "            \"phase\": \"init\",\n",
    "        })\n",
    "\n",
    "    # 2) BO loop\n",
    "    gp = make_gp(seed=seed)\n",
    "\n",
    "    for it in range(init_points, budget):\n",
    "        gp.fit(X, y)\n",
    "\n",
    "        y_best = float(np.max(y))\n",
    "\n",
    "        # Propose next\n",
    "        x_next = propose_next_point(\n",
    "            rng=rng,\n",
    "            gp=gp,\n",
    "            bounds=bounds,\n",
    "            acq=acq,\n",
    "            y_best=y_best,\n",
    "            n_candidates=n_candidates,\n",
    "        )\n",
    "\n",
    "        # Evaluate simulator\n",
    "        y_next, cost_next = simulator_objective(rng, x_next, cfg, noisy=True, shift=0.0)\n",
    "\n",
    "        # Append\n",
    "        X = np.vstack([X, x_next])\n",
    "        y = np.concatenate([y, y_next])\n",
    "        cost = np.concatenate([cost, cost_next])\n",
    "\n",
    "        trace_rows.append({\n",
    "            \"iter\": it,\n",
    "            \"x1\": float(x_next[0, 0]),\n",
    "            \"x2\": float(x_next[0, 1]),\n",
    "            \"y\": float(y_next[0]),\n",
    "            \"cost\": float(cost_next[0]),\n",
    "            \"phase\": \"bo\",\n",
    "            \"best_y_before\": y_best,\n",
    "        })\n",
    "\n",
    "    # Final fit\n",
    "    gp.fit(X, y)\n",
    "    y_best = float(np.max(y))\n",
    "    best_idx = int(np.argmax(y))\n",
    "    x_best = X[best_idx]\n",
    "\n",
    "    # 3) Optional OOD shift evaluation\n",
    "    shift_report = None\n",
    "    if shift_test:\n",
    "        x_test = x_best.reshape(1, 2)\n",
    "        y_in, _ = simulator_objective(rng, x_test, cfg, noisy=False, shift=0.0)\n",
    "        y_shifted, _ = simulator_objective(rng, x_test, cfg, noisy=False, shift=0.35)\n",
    "        shift_report = {\n",
    "            \"best_x\": {\"x1\": float(x_best[0]), \"x2\": float(x_best[1])},\n",
    "            \"score_in_domain_noiseless\": float(y_in[0]),\n",
    "            \"score_shifted_noiseless\": float(y_shifted[0]),\n",
    "            \"drop_due_to_shift\": float(y_in[0] - y_shifted[0]),\n",
    "        }\n",
    "\n",
    "    # 4) Save tables\n",
    "    trace_df = pd.DataFrame(trace_rows)\n",
    "    trace_df.to_csv(os.path.join(outdir, \"tables\", \"bo_trace.csv\"), index=False)\n",
    "\n",
    "    # 5) Produce scientific plots\n",
    "    figdir = os.path.join(outdir, \"figures\")\n",
    "\n",
    "    # Convergence curve\n",
    "    best_curve = np.maximum.accumulate(trace_df[\"y\"].to_numpy())\n",
    "    plt.figure(figsize=(7.0, 4.8))\n",
    "    plt.plot(trace_df[\"iter\"], best_curve, marker=\"o\", linewidth=1.2)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Best observed objective\")\n",
    "    plt.title(f\"Bayesian Optimisation Convergence ({acq.upper()})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"convergence.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 2D surrogate map (mean and std)\n",
    "    grid_n = 140\n",
    "    x1g = np.linspace(cfg.x1_bounds[0], cfg.x1_bounds[1], grid_n)\n",
    "    x2g = np.linspace(cfg.x2_bounds[0], cfg.x2_bounds[1], grid_n)\n",
    "    xx1, xx2 = np.meshgrid(x1g, x2g)\n",
    "    Xgrid = np.column_stack([xx1.ravel(), xx2.ravel()])\n",
    "\n",
    "    mu, std = gp.predict(Xgrid, return_std=True)\n",
    "    MU = mu.reshape(grid_n, grid_n)\n",
    "    STD = std.reshape(grid_n, grid_n)\n",
    "\n",
    "    # Training points overlay\n",
    "    plt.figure(figsize=(7.0, 5.6))\n",
    "    plt.imshow(\n",
    "        MU,\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[cfg.x1_bounds[0], cfg.x1_bounds[1], cfg.x2_bounds[0], cfg.x2_bounds[1]],\n",
    "    )\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=18)\n",
    "    plt.scatter(x_best[0], x_best[1], s=80, marker=\"x\")\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(\"GP Surrogate Mean (sampled points + best point)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"gp_mean_map.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(7.0, 5.6))\n",
    "    plt.imshow(\n",
    "        STD,\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[cfg.x1_bounds[0], cfg.x1_bounds[1], cfg.x2_bounds[0], cfg.x2_bounds[1]],\n",
    "    )\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=18)\n",
    "    plt.scatter(x_best[0], x_best[1], s=80, marker=\"x\")\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(\"GP Surrogate Uncertainty (std)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"gp_std_map.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # Acquisition map\n",
    "    if acq.lower() == \"ei\":\n",
    "        acq_vals = expected_improvement(mu, std, best_y=float(np.max(y)), xi=0.01)\n",
    "    else:\n",
    "        acq_vals = upper_confidence_bound(mu, std, beta=2.2)\n",
    "    ACQ = acq_vals.reshape(grid_n, grid_n)\n",
    "\n",
    "    plt.figure(figsize=(7.0, 5.6))\n",
    "    plt.imshow(\n",
    "        ACQ,\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[cfg.x1_bounds[0], cfg.x1_bounds[1], cfg.x2_bounds[0], cfg.x2_bounds[1]],\n",
    "    )\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=18)\n",
    "    plt.scatter(x_best[0], x_best[1], s=80, marker=\"x\")\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(f\"Acquisition Map ({acq.upper()})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"acquisition_map.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # Compare BO vs random search baseline (fair budget)\n",
    "    Xrand = np.column_stack([\n",
    "        rng.uniform(cfg.x1_bounds[0], cfg.x1_bounds[1], size=budget),\n",
    "        rng.uniform(cfg.x2_bounds[0], cfg.x2_bounds[1], size=budget),\n",
    "    ])\n",
    "    yrand, _ = simulator_objective(rng, Xrand, cfg, noisy=True, shift=0.0)\n",
    "    best_rand = np.maximum.accumulate(yrand)\n",
    "\n",
    "    plt.figure(figsize=(7.0, 4.8))\n",
    "    plt.plot(np.arange(budget), best_curve[:budget], marker=\"o\", linewidth=1.2, label=f\"BO ({acq.upper()})\")\n",
    "    plt.plot(np.arange(budget), best_rand, marker=\"o\", linewidth=1.2, label=\"Random search\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Best observed objective\")\n",
    "    plt.title(\"Sample Efficiency: Bayesian Optimisation vs Random\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"bo_vs_random.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 6) Save metrics report\n",
    "    metrics = {\n",
    "        \"seed\": seed,\n",
    "        \"budget\": budget,\n",
    "        \"init_points\": init_points,\n",
    "        \"acquisition\": acq,\n",
    "        \"best_y_observed\": float(y_best),\n",
    "        \"best_x_observed\": {\"x1\": float(x_best[0]), \"x2\": float(x_best[1])},\n",
    "        \"random_baseline_best_y\": float(np.max(yrand)),\n",
    "        \"efficiency_gap_best_y\": float(y_best - np.max(yrand)),\n",
    "        \"gp_kernel\": str(gp.kernel_),\n",
    "        \"ood_shift_check\": shift_report,\n",
    "    }\n",
    "    save_json(metrics, os.path.join(outdir, \"metrics_report.json\"))\n",
    "\n",
    "    # 7) Print concise summary\n",
    "    print(\"\\n=== GP Bayesian Optimisation PoC ===\")\n",
    "    print(f\"Outputs saved to: {os.path.abspath(outdir)}\\n\")\n",
    "    print(\"Best found design:\")\n",
    "    print(\"  x1 =\", float(x_best[0]))\n",
    "    print(\"  x2 =\", float(x_best[1]))\n",
    "    print(\"  best y =\", float(y_best))\n",
    "    print(\"\\nRandom search best y (same budget):\", float(np.max(yrand)))\n",
    "    print(\"Efficiency gap (BO - random):\", float(y_best - np.max(yrand)))\n",
    "\n",
    "    if shift_report is not None:\n",
    "        print(\"\\nOOD shift check at best point (noiseless):\")\n",
    "        print(\"  in-domain score :\", shift_report[\"score_in_domain_noiseless\"])\n",
    "        print(\"  shifted score   :\", shift_report[\"score_shifted_noiseless\"])\n",
    "        print(\"  drop            :\", shift_report[\"drop_due_to_shift\"])\n",
    "\n",
    "    print(\"\\nKey figures written to outputs/figures/\")\n",
    "    print(\"Key tables written to outputs/tables/\\n\")\n",
    "\n",
    "\n",
    "def make_argparser() -> argparse.ArgumentParser:\n",
    "    p = argparse.ArgumentParser(description=\"UH Data Science PoC: Gaussian Process Bayesian Optimisation\")\n",
    "    p.add_argument(\"--outdir\", type=str, default=\"uh_bo_outputs\", help=\"Output directory\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "    p.add_argument(\"--budget\", type=int, default=60, help=\"Total number of evaluations (including init)\")\n",
    "    p.add_argument(\"--init_points\", type=int, default=12, help=\"Initial points before BO loop\")\n",
    "    p.add_argument(\"--acq\", type=str, default=\"ei\", choices=[\"ei\", \"ucb\"], help=\"Acquisition function\")\n",
    "    p.add_argument(\"--n_candidates\", type=int, default=6000, help=\"Random candidates for acquisition maximisation\")\n",
    "    p.add_argument(\"--no_shift_test\", action=\"store_true\", help=\"Disable OOD shift check\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = make_argparser()\n",
    "    args, _ = parser.parse_known_args()  # Robust in Jupyter + terminal (ignores -f kernel.json)\n",
    "    run_bayesopt(\n",
    "        outdir=args.outdir,\n",
    "        seed=args.seed,\n",
    "        budget=args.budget,\n",
    "        init_points=args.init_points,\n",
    "        acq=args.acq,\n",
    "        n_candidates=args.n_candidates,\n",
    "        shift_test=(not args.no_shift_test),\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4f17e0-4c94-4296-b73d-ba9471b5cb76",
   "metadata": {},
   "source": [
    "# Multi-Objective Bayesian Optimisation (Pareto Front + Hypervolume Tracking) for a Physics-Style Expensive Simulator\n",
    "\n",
    "## What this script demonstrates:\n",
    "    1) Multi-objective optimisation:\n",
    "         - Objective 1: maximise \"physics performance\"\n",
    "         - Objective 2: minimise \"experiment cost\"\n",
    "       (trade-off is unavoidable and realistic)\n",
    "    2) Bayesian optimisation loop:\n",
    "         - GP surrogate models for each objective\n",
    "         - Scalarisation sampling to explore Pareto front\n",
    "         - Exploration bonus using GP uncertainty\n",
    "    3) Pareto front estimation:\n",
    "         - non-dominated sorting\n",
    "         - hypervolume metric improvement over time\n",
    "    4) Scientific reporting:\n",
    "         - Pareto plots\n",
    "         - hypervolume curve\n",
    "         - trace tables + JSON metrics\n",
    "\n",
    "## Dependencies:\n",
    "    numpy, pandas, matplotlib, scikit-learn\n",
    "\n",
    "## Run:\n",
    "    python uh_mobo_pareto_physics_poc.py --outdir uh_mobo_outputs --budget 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c952310-dc3a-49e6-84e0-2beb986eed01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Multi-objective Bayesian Optimisation PoC ===\n",
      "Outputs saved to: /Users/petalc01/Data Science Hertfordshire/uh_mobo_outputs\n",
      "\n",
      "Pareto front size: 12\n",
      "Final hypervolume: 132.9772940947282\n",
      "Best performance observed: 1.8976758678346248\n",
      "Minimum cost observed: -0.020578763202621177\n",
      "\n",
      "Key figures written to outputs/figures/\n",
      "Key tables written to outputs/tables/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Silence ONLY the specific warning you reported (optional but clean).\n",
    "# Keeping this is harmless even after fixing the kernel bounds.\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*parameter k2__noise_level is close to the specified lower bound.*\",\n",
    "    category=ConvergenceWarning,\n",
    "    module=r\"sklearn\\.gaussian_process\\.kernels\",\n",
    ")\n",
    "\n",
    "\n",
    "# Repro utilities\n",
    "\n",
    "def set_seed(seed: int = 42) -> np.random.Generator:\n",
    "    return np.random.default_rng(seed)\n",
    "\n",
    "\n",
    "def ensure_outdir(outdir: str) -> str:\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(outdir, \"figures\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(outdir, \"tables\"), exist_ok=True)\n",
    "    return outdir\n",
    "\n",
    "\n",
    "def save_json(obj: Dict, path: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "\n",
    "# Physics-inspired multi-objective simulator\n",
    "\n",
    "@dataclass\n",
    "class SimSettings:\n",
    "    x1_bounds: Tuple[float, float] = (-2.0, 2.0)\n",
    "    x2_bounds: Tuple[float, float] = (-2.0, 2.0)\n",
    "    noise_sigma: float = 0.08\n",
    "    constraint_strength: float = 7.5\n",
    "\n",
    "\n",
    "def simulator_multiobjective(\n",
    "    rng: np.random.Generator,\n",
    "    X: np.ndarray,\n",
    "    cfg: SimSettings,\n",
    "    noisy: bool = True,\n",
    "    shift: float = 0.0,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        f1: performance score to MAXIMISE\n",
    "        f2: experiment cost to MINIMISE\n",
    "    \"\"\"\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "\n",
    "    # Physics performance (multi-modal)\n",
    "    perf = (\n",
    "        1.8 * np.sin(2.6 * (x1 + shift)) * np.cos(2.2 * (x2 - 0.25 * shift))\n",
    "        + 0.95 * np.exp(-((x1 - 0.9 + 0.2 * shift) ** 2 + (x2 + 0.55) ** 2) / 0.35)\n",
    "        + 1.10 * np.exp(-((x1 + 1.05) ** 2 + (x2 - 0.95 + 0.1 * shift) ** 2) / 0.25)\n",
    "        - 0.12 * (x1**2 + x2**2)\n",
    "    )\n",
    "\n",
    "    # Experiment cost (positive)\n",
    "    cost = 0.55 * (np.abs(x1) ** 1.3) + 0.45 * (np.abs(x2) ** 1.6)\n",
    "\n",
    "    # Feasibility/stability penalty (subtract from performance)\n",
    "    stability_limit = 1.2 - 0.5 * (x1**2)\n",
    "    violation = np.maximum(0.0, x2 - stability_limit)\n",
    "    penalty = cfg.constraint_strength * (violation ** 2)\n",
    "\n",
    "    f1_clean = perf - penalty\n",
    "    f2_clean = cost\n",
    "\n",
    "    if noisy:\n",
    "        f1 = f1_clean + rng.normal(0.0, cfg.noise_sigma, size=f1_clean.shape)\n",
    "        f2 = f2_clean + rng.normal(0.0, 0.02, size=f2_clean.shape)  # small cost noise\n",
    "    else:\n",
    "        f1, f2 = f1_clean, f2_clean\n",
    "\n",
    "    return f1.astype(float), f2.astype(float)\n",
    "\n",
    "\n",
    "def latin_hypercube_2d(rng: np.random.Generator, n: int, bounds: Tuple[Tuple[float, float], Tuple[float, float]]) -> np.ndarray:\n",
    "    (a1, b1), (a2, b2) = bounds\n",
    "    u1 = (np.arange(n) + rng.random(n)) / n\n",
    "    u2 = (np.arange(n) + rng.random(n)) / n\n",
    "    rng.shuffle(u1)\n",
    "    rng.shuffle(u2)\n",
    "    x1 = a1 + (b1 - a1) * u1\n",
    "    x2 = a2 + (b2 - a2) * u2\n",
    "    return np.vstack([x1, x2]).T\n",
    "\n",
    "\n",
    "# Pareto and hypervolume\n",
    "\n",
    "def pareto_mask_maxmin(f1: np.ndarray, f2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Non-dominated points where:\n",
    "        f1: maximise\n",
    "        f2: minimise\n",
    "    A point i is dominated if there exists j with:\n",
    "        f1_j >= f1_i and f2_j <= f2_i, with at least one strict.\n",
    "    \"\"\"\n",
    "    n = len(f1)\n",
    "    dominated = np.zeros(n, dtype=bool)\n",
    "\n",
    "    for i in range(n):\n",
    "        if dominated[i]:\n",
    "            continue\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if (f1[j] >= f1[i] and f2[j] <= f2[i]) and (f1[j] > f1[i] or f2[j] < f2[i]):\n",
    "                dominated[i] = True\n",
    "                break\n",
    "\n",
    "    return ~dominated\n",
    "\n",
    "\n",
    "def hypervolume_2d_maxmin(f1: np.ndarray, f2: np.ndarray, ref: Tuple[float, float]) -> float:\n",
    "    \"\"\"\n",
    "    Hypervolume for 2D with:\n",
    "        maximise f1, minimise f2\n",
    "    Reference point ref = (f1_ref, f2_ref) should be \"worse\" than all points:\n",
    "        f1_ref small, f2_ref large.\n",
    "    We compute area dominated by Pareto front to the reference.\n",
    "\n",
    "    Implementation:\n",
    "        Sort Pareto points by f2 ascending (lower cost better),\n",
    "        accumulate rectangles in (f1, f2) plane.\n",
    "    \"\"\"\n",
    "    mask = pareto_mask_maxmin(f1, f2)\n",
    "    p1 = f1[mask]\n",
    "    p2 = f2[mask]\n",
    "\n",
    "    if len(p1) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # sort by cost increasing\n",
    "    order = np.argsort(p2)\n",
    "    p1 = p1[order]\n",
    "    p2 = p2[order]\n",
    "\n",
    "    f1_ref, f2_ref = ref\n",
    "    hv = 0.0\n",
    "    best_f1_so_far = f1_ref\n",
    "\n",
    "    # Walk along increasing cost, add slices where f1 improves\n",
    "    for i in range(len(p1)):\n",
    "        # width in f1 direction: improvement over best_f1_so_far\n",
    "        width = max(0.0, p1[i] - best_f1_so_far)\n",
    "        # height in f2 direction: from this cost to reference cost (since we minimise f2)\n",
    "        height = max(0.0, f2_ref - p2[i])\n",
    "        hv += width * height\n",
    "        best_f1_so_far = max(best_f1_so_far, p1[i])\n",
    "\n",
    "    return float(hv)\n",
    "\n",
    "\n",
    "# GP models\n",
    "\n",
    "def make_gp(seed: int = 42) -> GaussianProcessRegressor:\n",
    "    kernel = (\n",
    "        ConstantKernel(1.0, (1e-2, 1e2))\n",
    "        * RBF(length_scale=np.ones(2), length_scale_bounds=(1e-2, 1e2))\n",
    "        + WhiteKernel(noise_level=1e-2, noise_level_bounds=(1e-6, 1e1))\n",
    "    )\n",
    "    return GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        normalize_y=True,\n",
    "        random_state=seed,\n",
    "        n_restarts_optimizer=4,\n",
    "    )\n",
    "\n",
    "\n",
    "# MOBO acquisition (scalarisation + uncertainty bonus)\n",
    "\n",
    "def propose_next_point_mobo(\n",
    "    rng: np.random.Generator,\n",
    "    gp1: GaussianProcessRegressor,\n",
    "    gp2: GaussianProcessRegressor,\n",
    "    bounds: Tuple[Tuple[float, float], Tuple[float, float]],\n",
    "    n_candidates: int = 8000,\n",
    "    explore_bonus: float = 0.15,\n",
    ") -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Multi-objective acquisition via random scalarisation.\n",
    "\n",
    "    We sample a weight w in (0,1):\n",
    "        score = w * (standardised f1) - (1-w) * (standardised f2) + explore_bonus * uncertainty\n",
    "\n",
    "    Since f1 is maximised and f2 minimised, we subtract f2 component.\n",
    "    \"\"\"\n",
    "    (a1, b1), (a2, b2) = bounds\n",
    "    Xcand = np.column_stack([\n",
    "        rng.uniform(a1, b1, size=n_candidates),\n",
    "        rng.uniform(a2, b2, size=n_candidates),\n",
    "    ])\n",
    "\n",
    "    mu1, std1 = gp1.predict(Xcand, return_std=True)\n",
    "    mu2, std2 = gp2.predict(Xcand, return_std=True)\n",
    "\n",
    "    # Standardise predicted means to comparable scales\n",
    "    m1 = (mu1 - np.mean(mu1)) / (np.std(mu1) + 1e-12)\n",
    "    m2 = (mu2 - np.mean(mu2)) / (np.std(mu2) + 1e-12)\n",
    "\n",
    "    # exploration proxy: combine uncertainties\n",
    "    unc = (std1 / (np.mean(std1) + 1e-12)) + (std2 / (np.mean(std2) + 1e-12))\n",
    "\n",
    "    # random scalarisation weight\n",
    "    w = float(rng.uniform(0.15, 0.85))\n",
    "    score = w * m1 - (1.0 - w) * m2 + explore_bonus * unc\n",
    "\n",
    "    idx = int(np.argmax(score))\n",
    "    meta = {\"w\": w, \"explore_bonus\": explore_bonus}\n",
    "    return Xcand[idx:idx+1, :], meta\n",
    "\n",
    "\n",
    "# Experiment\n",
    "\n",
    "def run_mobo(\n",
    "    outdir: str,\n",
    "    seed: int = 42,\n",
    "    budget: int = 80,\n",
    "    init_points: int = 14,\n",
    "    n_candidates: int = 8000,\n",
    "    explore_bonus: float = 0.15,\n",
    ") -> None:\n",
    "    rng = set_seed(seed)\n",
    "    ensure_outdir(outdir)\n",
    "\n",
    "    cfg = SimSettings()\n",
    "    bounds = (cfg.x1_bounds, cfg.x2_bounds)\n",
    "\n",
    "    # Initialise with LHS\n",
    "    X = latin_hypercube_2d(rng, init_points, bounds)\n",
    "    f1, f2 = simulator_multiobjective(rng, X, cfg, noisy=True, shift=0.0)\n",
    "\n",
    "    trace = []\n",
    "    for i in range(init_points):\n",
    "        trace.append({\n",
    "            \"iter\": i,\n",
    "            \"x1\": float(X[i, 0]),\n",
    "            \"x2\": float(X[i, 1]),\n",
    "            \"f1_perf\": float(f1[i]),\n",
    "            \"f2_cost\": float(f2[i]),\n",
    "            \"phase\": \"init\",\n",
    "        })\n",
    "\n",
    "    # MOBO loop\n",
    "    gp1 = make_gp(seed=seed)\n",
    "    gp2 = make_gp(seed=seed)\n",
    "\n",
    "    # choose reference point for hypervolume (worse than likely observed)\n",
    "    # we can set:\n",
    "    #   f1_ref: slightly below min observed\n",
    "    #   f2_ref: slightly above max observed\n",
    "    f1_ref = float(np.min(f1) - 0.5)\n",
    "    f2_ref = float(np.max(f2) + 0.5)\n",
    "    hv_curve = []\n",
    "\n",
    "    for it in range(init_points, budget):\n",
    "        gp1.fit(X, f1)\n",
    "        gp2.fit(X, f2)\n",
    "\n",
    "        # hypervolume progress\n",
    "        hv = hypervolume_2d_maxmin(f1, f2, ref=(f1_ref, f2_ref))\n",
    "        hv_curve.append(hv)\n",
    "\n",
    "        x_next, meta = propose_next_point_mobo(\n",
    "            rng=rng,\n",
    "            gp1=gp1,\n",
    "            gp2=gp2,\n",
    "            bounds=bounds,\n",
    "            n_candidates=n_candidates,\n",
    "            explore_bonus=explore_bonus,\n",
    "        )\n",
    "\n",
    "        f1_next, f2_next = simulator_multiobjective(rng, x_next, cfg, noisy=True, shift=0.0)\n",
    "\n",
    "        X = np.vstack([X, x_next])\n",
    "        f1 = np.concatenate([f1, f1_next])\n",
    "        f2 = np.concatenate([f2, f2_next])\n",
    "\n",
    "        trace.append({\n",
    "            \"iter\": it,\n",
    "            \"x1\": float(x_next[0, 0]),\n",
    "            \"x2\": float(x_next[0, 1]),\n",
    "            \"f1_perf\": float(f1_next[0]),\n",
    "            \"f2_cost\": float(f2_next[0]),\n",
    "            \"phase\": \"mobo\",\n",
    "            \"scalar_weight_w\": meta[\"w\"],\n",
    "            \"explore_bonus\": meta[\"explore_bonus\"],\n",
    "            \"hypervolume_before\": hv,\n",
    "        })\n",
    "\n",
    "    # final HV\n",
    "    hv_final = hypervolume_2d_maxmin(f1, f2, ref=(f1_ref, f2_ref))\n",
    "    hv_curve.append(hv_final)\n",
    "\n",
    "    # Save trace\n",
    "    trace_df = pd.DataFrame(trace)\n",
    "    trace_df.to_csv(os.path.join(outdir, \"tables\", \"mobo_trace.csv\"), index=False)\n",
    "\n",
    "    # Pareto front\n",
    "    mask = pareto_mask_maxmin(f1, f2)\n",
    "    pareto_df = pd.DataFrame({\n",
    "        \"x1\": X[mask, 0],\n",
    "        \"x2\": X[mask, 1],\n",
    "        \"f1_perf\": f1[mask],\n",
    "        \"f2_cost\": f2[mask],\n",
    "    }).sort_values(\"f2_cost\").reset_index(drop=True)\n",
    "\n",
    "    pareto_df.to_csv(os.path.join(outdir, \"tables\", \"pareto_front.csv\"), index=False)\n",
    "\n",
    "    # Figures\n",
    "    figdir = os.path.join(outdir, \"figures\")\n",
    "\n",
    "    # Pareto scatter\n",
    "    plt.figure(figsize=(7.0, 5.2))\n",
    "    plt.scatter(f2, f1, s=18, label=\"All evaluated\")\n",
    "    plt.scatter(pareto_df[\"f2_cost\"], pareto_df[\"f1_perf\"], s=35, label=\"Pareto front\")\n",
    "    plt.xlabel(\"Cost (minimise)\")\n",
    "    plt.ylabel(\"Performance (maximise)\")\n",
    "    plt.title(\"Pareto Front Discovery (Multi-objective Optimisation)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"pareto_front.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # Hypervolume curve\n",
    "    plt.figure(figsize=(7.0, 4.8))\n",
    "    x_it = np.arange(init_points, budget + 1)\n",
    "    plt.plot(x_it, hv_curve, marker=\"o\", linewidth=1.2)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Hypervolume (higher is better)\")\n",
    "    plt.title(\"Hypervolume Improvement over Iterations\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"hypervolume_curve.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # Pareto in design space (x1,x2)\n",
    "    plt.figure(figsize=(7.0, 5.2))\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=18, label=\"All evaluated\")\n",
    "    plt.scatter(pareto_df[\"x1\"], pareto_df[\"x2\"], s=40, label=\"Pareto designs\")\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(\"Design Space, evaluated points and Pareto designs\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"design_space_pareto.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        \"seed\": seed,\n",
    "        \"budget\": budget,\n",
    "        \"init_points\": init_points,\n",
    "        \"n_candidates\": n_candidates,\n",
    "        \"explore_bonus\": explore_bonus,\n",
    "        \"hypervolume_ref_point\": {\"f1_ref\": f1_ref, \"f2_ref\": f2_ref},\n",
    "        \"hypervolume_final\": float(hv_final),\n",
    "        \"n_pareto_points\": int(len(pareto_df)),\n",
    "        \"best_perf_overall\": float(np.max(f1)),\n",
    "        \"min_cost_overall\": float(np.min(f2)),\n",
    "    }\n",
    "    save_json(metrics, os.path.join(outdir, \"metrics_report.json\"))\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n=== Multi-objective Bayesian Optimisation PoC ===\")\n",
    "    print(f\"Outputs saved to: {os.path.abspath(outdir)}\\n\")\n",
    "    print(\"Pareto front size:\", len(pareto_df))\n",
    "    print(\"Final hypervolume:\", float(hv_final))\n",
    "    print(\"Best performance observed:\", float(np.max(f1)))\n",
    "    print(\"Minimum cost observed:\", float(np.min(f2)))\n",
    "    print(\"\\nKey figures written to outputs/figures/\")\n",
    "    print(\"Key tables written to outputs/tables/\\n\")\n",
    "\n",
    "\n",
    "def make_argparser() -> argparse.ArgumentParser:\n",
    "    p = argparse.ArgumentParser(description=\"UH MOBO PoC: Pareto front + hypervolume tracking\")\n",
    "    p.add_argument(\"--outdir\", type=str, default=\"uh_mobo_outputs\", help=\"Output directory\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "    p.add_argument(\"--budget\", type=int, default=80, help=\"Total evaluations (including init)\")\n",
    "    p.add_argument(\"--init_points\", type=int, default=14, help=\"Initial LHS points\")\n",
    "    p.add_argument(\"--n_candidates\", type=int, default=8000, help=\"Random candidates per iteration\")\n",
    "    p.add_argument(\"--explore_bonus\", type=float, default=0.15, help=\"Exploration strength\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = make_argparser()\n",
    "    args, _ = parser.parse_known_args()  # Robust in Jupyter + terminal (ignores -f kernel.json)\n",
    "    run_mobo(\n",
    "        outdir=args.outdir,\n",
    "        seed=args.seed,\n",
    "        budget=args.budget,\n",
    "        init_points=args.init_points,\n",
    "        n_candidates=args.n_candidates,\n",
    "        explore_bonus=args.explore_bonus,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8435ed93-e40f-4301-a41d-5f8d636152fc",
   "metadata": {},
   "source": [
    "# Multi-Objective Bayesian Optimisation (MOBO++) for Physics-Style Simulator: Pareto Front + Pareto Ranks + Hypervolume Contribution + Epsilon-Constraint + Batch BO\n",
    "\n",
    "## Objectives:\n",
    "    f1: maximise performance\n",
    "    f2: minimise cost\n",
    "\n",
    "## What this script demonstrates:\n",
    "    1) A realistic multi-objective scientific optimisation problem (performance vs cost).\n",
    "    2) GP surrogates + uncertainty, active learning.\n",
    "    3) Batch BO (K proposals per iteration) for realistic experimental throughput.\n",
    "    4) Pareto ranks (non-dominated sorting levels).\n",
    "    5) Hypervolume tracking over time.\n",
    "    6) Hypervolume contribution of Pareto points (impact of each design).\n",
    "    7) Epsilon-constraint extraction: best performance under specified cost thresholds.\n",
    "    8) Reproducible artefacts: figures, tables, JSON metrics.\n",
    "\n",
    "## Dependencies:\n",
    "    numpy, pandas, matplotlib, scikit-learn\n",
    "\n",
    "## Run:\n",
    "    python uh_mobo_pareto_physics_poc_plus.py --outdir uh_mobo_plus --budget 100 --batch_size 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0a990ad-45a0-4b3e-ab79-79a9f35dbc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MOBO++ (Pareto + Ranks + HV contribution + Epsilon constraint + Batch BO) ===\n",
      "Outputs saved to: /Users/petalc01/Data Science Hertfordshire/uh_mobo_plus\n",
      "\n",
      "Final hypervolume: 83.85202924240279\n",
      "Pareto front size: 15\n",
      "Best performance observed: 1.3893459905783005\n",
      "Minimum cost observed: -0.04151575088423652\n",
      "\n",
      "Top Pareto point by hypervolume contribution:\n",
      "  index: 89\n",
      "  x1,x2: -0.01860984679671196 0.1094652212937901\n",
      "  perf: 0.13141950343637387  cost: -0.04151575088423652\n",
      "  hv contrib: 0.29874602106409043\n",
      "\n",
      "OOD shift check (top Pareto points, noiseless):\n",
      "  worst perf drop: -0.5002095879411976 at index 83\n",
      "\n",
      "Figures -> uh_mobo_plus/figures\n",
      "Tables  -> uh_mobo_plus/tables \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Silence ONLY the specific warning you reported (optional but clean).\n",
    "# Keeping this is harmless even after fixing the kernel bounds.\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*parameter k2__noise_level is close to the specified lower bound.*\",\n",
    "    category=ConvergenceWarning,\n",
    "    module=r\"sklearn\\.gaussian_process\\.kernels\",\n",
    ")\n",
    "\n",
    "\n",
    "# Repro utilities\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> np.random.Generator:\n",
    "    return np.random.default_rng(seed)\n",
    "\n",
    "\n",
    "def ensure_outdir(outdir: str) -> str:\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(outdir, \"figures\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(outdir, \"tables\"), exist_ok=True)\n",
    "    return outdir\n",
    "\n",
    "\n",
    "def save_json(obj: Dict, path: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "\n",
    "# Physics-inspired simulator\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SimSettings:\n",
    "    x1_bounds: Tuple[float, float] = (-2.0, 2.0)\n",
    "    x2_bounds: Tuple[float, float] = (-2.0, 2.0)\n",
    "    noise_sigma: float = 0.08\n",
    "    constraint_strength: float = 7.5\n",
    "\n",
    "\n",
    "def simulator_multiobjective(\n",
    "    rng: np.random.Generator,\n",
    "    X: np.ndarray,\n",
    "    cfg: SimSettings,\n",
    "    noisy: bool = True,\n",
    "    shift: float = 0.0,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        f1: performance score to MAXIMISE\n",
    "        f2: experiment cost to MINIMISE\n",
    "    \"\"\"\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "\n",
    "    # Performance landscape: resonance/interference + multiple peaks\n",
    "    perf = (\n",
    "        1.8 * np.sin(2.6 * (x1 + shift)) * np.cos(2.2 * (x2 - 0.25 * shift))\n",
    "        + 0.95 * np.exp(-((x1 - 0.9 + 0.2 * shift) ** 2 + (x2 + 0.55) ** 2) / 0.35)\n",
    "        + 1.10 * np.exp(-((x1 + 1.05) ** 2 + (x2 - 0.95 + 0.1 * shift) ** 2) / 0.25)\n",
    "        - 0.12 * (x1**2 + x2**2)\n",
    "    )\n",
    "\n",
    "    # Cost is always positive, grows with magnitude\n",
    "    cost = 0.55 * (np.abs(x1) ** 1.3) + 0.45 * (np.abs(x2) ** 1.6)\n",
    "\n",
    "    # Soft stability constraint, penalise infeasible x2 above a curved boundary\n",
    "    stability_limit = 1.2 - 0.5 * (x1**2)\n",
    "    violation = np.maximum(0.0, x2 - stability_limit)\n",
    "    penalty = cfg.constraint_strength * (violation ** 2)\n",
    "\n",
    "    f1_clean = perf - penalty\n",
    "    f2_clean = cost\n",
    "\n",
    "    if noisy:\n",
    "        f1 = f1_clean + rng.normal(0.0, cfg.noise_sigma, size=f1_clean.shape)\n",
    "        f2 = f2_clean + rng.normal(0.0, 0.02, size=f2_clean.shape)\n",
    "    else:\n",
    "        f1, f2 = f1_clean, f2_clean\n",
    "\n",
    "    return f1.astype(float), f2.astype(float)\n",
    "\n",
    "\n",
    "def latin_hypercube_2d(\n",
    "    rng: np.random.Generator,\n",
    "    n: int,\n",
    "    bounds: Tuple[Tuple[float, float], Tuple[float, float]]\n",
    ") -> np.ndarray:\n",
    "    (a1, b1), (a2, b2) = bounds\n",
    "    u1 = (np.arange(n) + rng.random(n)) / n\n",
    "    u2 = (np.arange(n) + rng.random(n)) / n\n",
    "    rng.shuffle(u1)\n",
    "    rng.shuffle(u2)\n",
    "    x1 = a1 + (b1 - a1) * u1\n",
    "    x2 = a2 + (b2 - a2) * u2\n",
    "    return np.vstack([x1, x2]).T\n",
    "\n",
    "\n",
    "# Pareto tools\n",
    "\n",
    "\n",
    "def dominates_maxmin(a_f1: float, a_f2: float, b_f1: float, b_f2: float) -> bool:\n",
    "    \"\"\"\n",
    "    True if A dominates B for:\n",
    "        maximise f1\n",
    "        minimise f2\n",
    "    \"\"\"\n",
    "    return (a_f1 >= b_f1 and a_f2 <= b_f2) and (a_f1 > b_f1 or a_f2 < b_f2)\n",
    "\n",
    "\n",
    "def pareto_mask_maxmin(f1: np.ndarray, f2: np.ndarray) -> np.ndarray:\n",
    "    n = len(f1)\n",
    "    dominated = np.zeros(n, dtype=bool)\n",
    "    for i in range(n):\n",
    "        if dominated[i]:\n",
    "            continue\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if dominates_maxmin(f1[j], f2[j], f1[i], f2[i]):\n",
    "                dominated[i] = True\n",
    "                break\n",
    "    return ~dominated\n",
    "\n",
    "\n",
    "def fast_non_dominated_sort_maxmin(f1: np.ndarray, f2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns Pareto ranks:\n",
    "        rank 0: non-dominated front\n",
    "        rank 1: dominated only by rank 0 points\n",
    "        ...\n",
    "    \"\"\"\n",
    "    n = len(f1)\n",
    "    S = [[] for _ in range(n)]          # points dominated by i\n",
    "    n_dom = np.zeros(n, dtype=int)      # number of points dominating i\n",
    "    rank = np.full(n, -1, dtype=int)\n",
    "\n",
    "    fronts: List[List[int]] = [[]]\n",
    "\n",
    "    for p in range(n):\n",
    "        for q in range(n):\n",
    "            if p == q:\n",
    "                continue\n",
    "            if dominates_maxmin(f1[p], f2[p], f1[q], f2[q]):\n",
    "                S[p].append(q)\n",
    "            elif dominates_maxmin(f1[q], f2[q], f1[p], f2[p]):\n",
    "                n_dom[p] += 1\n",
    "\n",
    "        if n_dom[p] == 0:\n",
    "            rank[p] = 0\n",
    "            fronts[0].append(p)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(fronts) and len(fronts[i]) > 0:\n",
    "        next_front = []\n",
    "        for p in fronts[i]:\n",
    "            for q in S[p]:\n",
    "                n_dom[q] -= 1\n",
    "                if n_dom[q] == 0:\n",
    "                    rank[q] = i + 1\n",
    "                    next_front.append(q)\n",
    "        i += 1\n",
    "        fronts.append(next_front)\n",
    "\n",
    "    return rank\n",
    "\n",
    "\n",
    "def hypervolume_2d_maxmin(f1: np.ndarray, f2: np.ndarray, ref: Tuple[float, float]) -> float:\n",
    "    \"\"\"\n",
    "    2D hypervolume for maximise f1, minimise f2.\n",
    "    Reference ref = (f1_ref low, f2_ref high), a \"worst\" point.\n",
    "    \"\"\"\n",
    "    mask = pareto_mask_maxmin(f1, f2)\n",
    "    p1 = f1[mask]\n",
    "    p2 = f2[mask]\n",
    "\n",
    "    if len(p1) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    order = np.argsort(p2)  # by cost ascending\n",
    "    p1 = p1[order]\n",
    "    p2 = p2[order]\n",
    "\n",
    "    f1_ref, f2_ref = ref\n",
    "    hv = 0.0\n",
    "    best_f1 = f1_ref\n",
    "\n",
    "    for i in range(len(p1)):\n",
    "        width = max(0.0, p1[i] - best_f1)\n",
    "        height = max(0.0, f2_ref - p2[i])\n",
    "        hv += width * height\n",
    "        best_f1 = max(best_f1, p1[i])\n",
    "\n",
    "    return float(hv)\n",
    "\n",
    "\n",
    "def hypervolume_contributions_2d_maxmin(f1: np.ndarray, f2: np.ndarray, ref: Tuple[float, float]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Hypervolume contribution of each Pareto point:\n",
    "        contrib_i = HV(all Pareto points) - HV(all Pareto points except i)\n",
    "\n",
    "    Returns an array aligned to original indices, with 0 for non-Pareto points.\n",
    "    \"\"\"\n",
    "    mask = pareto_mask_maxmin(f1, f2)\n",
    "    idx_p = np.where(mask)[0]\n",
    "    contrib = np.zeros(len(f1), dtype=float)\n",
    "\n",
    "    if len(idx_p) == 0:\n",
    "        return contrib\n",
    "\n",
    "    hv_all = hypervolume_2d_maxmin(f1, f2, ref=ref)\n",
    "\n",
    "    for i in idx_p:\n",
    "        keep = np.ones(len(f1), dtype=bool)\n",
    "        keep[i] = False\n",
    "        hv_minus = hypervolume_2d_maxmin(f1[keep], f2[keep], ref=ref)\n",
    "        contrib[i] = max(0.0, hv_all - hv_minus)\n",
    "\n",
    "    return contrib\n",
    "\n",
    "\n",
    "def epsilon_constraint_best(f1: np.ndarray, f2: np.ndarray, eps_list: List[float]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each cost threshold eps, find the design with minimal cost <= eps\n",
    "    that gives maximum f1 (performance) among feasible points.\n",
    "\n",
    "    Output:\n",
    "        eps, best_perf, cost, index\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for eps in eps_list:\n",
    "        feasible = f2 <= eps\n",
    "        if not np.any(feasible):\n",
    "            rows.append({\"epsilon_cost\": eps, \"best_perf\": np.nan, \"cost\": np.nan, \"index\": -1})\n",
    "            continue\n",
    "        i_best = int(np.argmax(f1[feasible]))\n",
    "        idx = int(np.where(feasible)[0][i_best])\n",
    "        rows.append({\"epsilon_cost\": eps, \"best_perf\": float(f1[idx]), \"cost\": float(f2[idx]), \"index\": idx})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# GP models\n",
    "\n",
    "\n",
    "def make_gp(seed: int = 42) -> GaussianProcessRegressor:\n",
    "    kernel = (\n",
    "        ConstantKernel(1.0, (1e-2, 1e2))\n",
    "        * RBF(length_scale=np.ones(2), length_scale_bounds=(1e-2, 1e2))\n",
    "        + WhiteKernel(noise_level=1e-2, noise_level_bounds=(1e-6, 1e1))\n",
    "    )\n",
    "    return GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        normalize_y=True,\n",
    "        random_state=seed,\n",
    "        n_restarts_optimizer=4,\n",
    "    )\n",
    "\n",
    "\n",
    "# MOBO acquisition + Batch selection\n",
    "\n",
    "\n",
    "def propose_batch_mobo(\n",
    "    rng: np.random.Generator,\n",
    "    gp1: GaussianProcessRegressor,\n",
    "    gp2: GaussianProcessRegressor,\n",
    "    bounds: Tuple[Tuple[float, float], Tuple[float, float]],\n",
    "    batch_size: int = 4,\n",
    "    n_candidates: int = 12000,\n",
    "    explore_bonus: float = 0.18,\n",
    "    min_sep: float = 0.12,\n",
    ") -> Tuple[np.ndarray, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Batch MOBO via random scalarisation with greedy diversity:\n",
    "\n",
    "    score = w*m1 - (1-w)*m2 + explore_bonus*unc\n",
    "\n",
    "    Then select top points greedily subject to a minimum separation\n",
    "    in design space (to avoid redundant batch proposals).\n",
    "    \"\"\"\n",
    "    (a1, b1), (a2, b2) = bounds\n",
    "    Xcand = np.column_stack([\n",
    "        rng.uniform(a1, b1, size=n_candidates),\n",
    "        rng.uniform(a2, b2, size=n_candidates),\n",
    "    ])\n",
    "\n",
    "    mu1, std1 = gp1.predict(Xcand, return_std=True)\n",
    "    mu2, std2 = gp2.predict(Xcand, return_std=True)\n",
    "\n",
    "    m1 = (mu1 - np.mean(mu1)) / (np.std(mu1) + 1e-12)\n",
    "    m2 = (mu2 - np.mean(mu2)) / (np.std(mu2) + 1e-12)\n",
    "\n",
    "    unc = (std1 / (np.mean(std1) + 1e-12)) + (std2 / (np.mean(std2) + 1e-12))\n",
    "\n",
    "    # sample several scalarisations, take union of top candidates\n",
    "    # (gives better Pareto coverage)\n",
    "    scalar_weights = rng.uniform(0.10, 0.90, size=max(8, batch_size * 3))\n",
    "\n",
    "    # Score each candidate as max over sampled scalarisations\n",
    "    all_scores = np.full(n_candidates, -np.inf, dtype=float)\n",
    "    all_w = np.zeros(n_candidates, dtype=float)\n",
    "\n",
    "    for w in scalar_weights:\n",
    "        score = w * m1 - (1.0 - w) * m2 + explore_bonus * unc\n",
    "        better = score > all_scores\n",
    "        all_scores[better] = score[better]\n",
    "        all_w[better] = w\n",
    "\n",
    "    # Greedy selection with diversity constraint\n",
    "    order = np.argsort(-all_scores)\n",
    "    chosen = []\n",
    "    chosen_meta = []\n",
    "\n",
    "    for idx in order:\n",
    "        if len(chosen) >= batch_size:\n",
    "            break\n",
    "        x = Xcand[idx]\n",
    "\n",
    "        if len(chosen) == 0:\n",
    "            chosen.append(x)\n",
    "            chosen_meta.append({\"w\": float(all_w[idx]), \"score\": float(all_scores[idx])})\n",
    "            continue\n",
    "\n",
    "        # ensure separation from already chosen\n",
    "        dists = [float(np.linalg.norm(x - c)) for c in chosen]\n",
    "        if min(dists) >= min_sep:\n",
    "            chosen.append(x)\n",
    "            chosen_meta.append({\"w\": float(all_w[idx]), \"score\": float(all_scores[idx])})\n",
    "\n",
    "    # if not enough, relax separation\n",
    "    relax = min_sep\n",
    "    while len(chosen) < batch_size:\n",
    "        relax *= 0.85\n",
    "        for idx in order:\n",
    "            if len(chosen) >= batch_size:\n",
    "                break\n",
    "            x = Xcand[idx]\n",
    "            dists = [float(np.linalg.norm(x - c)) for c in chosen] if chosen else [np.inf]\n",
    "            if min(dists) >= relax:\n",
    "                chosen.append(x)\n",
    "                chosen_meta.append({\"w\": float(all_w[idx]), \"score\": float(all_scores[idx])})\n",
    "        if relax < 0.02:\n",
    "            break\n",
    "\n",
    "    Xnext = np.array(chosen, dtype=float)\n",
    "    return Xnext, chosen_meta\n",
    "\n",
    "\n",
    "# Main experiment\n",
    "\n",
    "\n",
    "def run_mobo_plus(\n",
    "    outdir: str,\n",
    "    seed: int = 42,\n",
    "    budget: int = 100,\n",
    "    init_points: int = 16,\n",
    "    batch_size: int = 4,\n",
    "    n_candidates: int = 12000,\n",
    "    explore_bonus: float = 0.18,\n",
    "    shift_test: bool = True,\n",
    ") -> None:\n",
    "    rng = set_seed(seed)\n",
    "    ensure_outdir(outdir)\n",
    "\n",
    "    cfg = SimSettings()\n",
    "    bounds = (cfg.x1_bounds, cfg.x2_bounds)\n",
    "\n",
    "    # Initialise with LHS\n",
    "    X = latin_hypercube_2d(rng, init_points, bounds)\n",
    "    f1, f2 = simulator_multiobjective(rng, X, cfg, noisy=True, shift=0.0)\n",
    "\n",
    "    # Hypervolume reference point (worse-than-observed)\n",
    "    f1_ref = float(np.min(f1) - 0.75)\n",
    "    f2_ref = float(np.max(f2) + 0.75)\n",
    "    hv_ref = (f1_ref, f2_ref)\n",
    "\n",
    "    trace = []\n",
    "    for i in range(init_points):\n",
    "        trace.append({\n",
    "            \"iter\": i,\n",
    "            \"x1\": float(X[i, 0]),\n",
    "            \"x2\": float(X[i, 1]),\n",
    "            \"f1_perf\": float(f1[i]),\n",
    "            \"f2_cost\": float(f2[i]),\n",
    "            \"phase\": \"init\",\n",
    "            \"batch_id\": -1,\n",
    "        })\n",
    "\n",
    "    gp1 = make_gp(seed=seed)\n",
    "    gp2 = make_gp(seed=seed)\n",
    "\n",
    "    hv_curve = []\n",
    "    step = init_points\n",
    "    batch_counter = 0\n",
    "\n",
    "    # Loop in batches\n",
    "    while step < budget:\n",
    "        gp1.fit(X, f1)\n",
    "        gp2.fit(X, f2)\n",
    "\n",
    "        hv_now = hypervolume_2d_maxmin(f1, f2, ref=hv_ref)\n",
    "        hv_curve.append({\"iter\": step, \"hypervolume\": hv_now})\n",
    "\n",
    "        # propose batch\n",
    "        Xnext, meta_list = propose_batch_mobo(\n",
    "            rng=rng,\n",
    "            gp1=gp1,\n",
    "            gp2=gp2,\n",
    "            bounds=bounds,\n",
    "            batch_size=min(batch_size, budget - step),\n",
    "            n_candidates=n_candidates,\n",
    "            explore_bonus=explore_bonus,\n",
    "            min_sep=0.12,\n",
    "        )\n",
    "\n",
    "        f1_next, f2_next = simulator_multiobjective(rng, Xnext, cfg, noisy=True, shift=0.0)\n",
    "\n",
    "        # append batch\n",
    "        X = np.vstack([X, Xnext])\n",
    "        f1 = np.concatenate([f1, f1_next])\n",
    "        f2 = np.concatenate([f2, f2_next])\n",
    "\n",
    "        # log trace\n",
    "        for k in range(len(Xnext)):\n",
    "            trace.append({\n",
    "                \"iter\": step + k,\n",
    "                \"x1\": float(Xnext[k, 0]),\n",
    "                \"x2\": float(Xnext[k, 1]),\n",
    "                \"f1_perf\": float(f1_next[k]),\n",
    "                \"f2_cost\": float(f2_next[k]),\n",
    "                \"phase\": \"mobo_batch\",\n",
    "                \"batch_id\": batch_counter,\n",
    "                \"scalar_weight_w\": meta_list[k].get(\"w\", np.nan),\n",
    "                \"acq_score\": meta_list[k].get(\"score\", np.nan),\n",
    "                \"hypervolume_before_batch\": hv_now,\n",
    "            })\n",
    "\n",
    "        step += len(Xnext)\n",
    "        batch_counter += 1\n",
    "\n",
    "    # Final HV\n",
    "    hv_final = hypervolume_2d_maxmin(f1, f2, ref=hv_ref)\n",
    "    hv_curve.append({\"iter\": budget, \"hypervolume\": hv_final})\n",
    "    hv_df = pd.DataFrame(hv_curve)\n",
    "\n",
    "    # Pareto mask + ranks + HV contributions\n",
    "    pmask = pareto_mask_maxmin(f1, f2)\n",
    "    ranks = fast_non_dominated_sort_maxmin(f1, f2)\n",
    "    hv_contrib = hypervolume_contributions_2d_maxmin(f1, f2, ref=hv_ref)\n",
    "\n",
    "    # Save main trace\n",
    "    trace_df = pd.DataFrame(trace)\n",
    "    trace_df.to_csv(os.path.join(outdir, \"tables\", \"mobo_plus_trace.csv\"), index=False)\n",
    "    hv_df.to_csv(os.path.join(outdir, \"tables\", \"hypervolume_trace.csv\"), index=False)\n",
    "\n",
    "    # Pareto table\n",
    "    pareto_idx = np.where(pmask)[0]\n",
    "    pareto_df = pd.DataFrame({\n",
    "        \"index\": pareto_idx,\n",
    "        \"x1\": X[pmask, 0],\n",
    "        \"x2\": X[pmask, 1],\n",
    "        \"f1_perf\": f1[pmask],\n",
    "        \"f2_cost\": f2[pmask],\n",
    "        \"pareto_rank\": ranks[pmask],\n",
    "        \"hv_contribution\": hv_contrib[pmask],\n",
    "    }).sort_values([\"f2_cost\", \"f1_perf\"], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    pareto_df.to_csv(os.path.join(outdir, \"tables\", \"pareto_front_plus.csv\"), index=False)\n",
    "\n",
    "    # Epsilon-constraint report: pick best perf under cost thresholds\n",
    "    cost_min, cost_max = float(np.min(f2)), float(np.max(f2))\n",
    "    eps_list = list(np.linspace(cost_min + 0.05, min(cost_max, cost_min + 1.8), 10))\n",
    "    eps_df = epsilon_constraint_best(f1, f2, eps_list=eps_list)\n",
    "    eps_df.to_csv(os.path.join(outdir, \"tables\", \"epsilon_constraint_report.csv\"), index=False)\n",
    "\n",
    "    # Optional OOD shift check on selected Pareto points (noiseless)\n",
    "    shift_report = None\n",
    "    if shift_test:\n",
    "        # evaluate top Pareto points by hypervolume contribution\n",
    "        topk = min(6, len(pareto_df))\n",
    "        if topk > 0:\n",
    "            cand_idx = pareto_df.sort_values(\"hv_contribution\", ascending=False).head(topk)[\"index\"].to_numpy().astype(int)\n",
    "            Xcand = X[cand_idx]\n",
    "            f1_in, f2_in = simulator_multiobjective(rng, Xcand, cfg, noisy=False, shift=0.0)\n",
    "            f1_shift, f2_shift = simulator_multiobjective(rng, Xcand, cfg, noisy=False, shift=0.35)\n",
    "\n",
    "            shift_report = []\n",
    "            for i, idx in enumerate(cand_idx):\n",
    "                shift_report.append({\n",
    "                    \"index\": int(idx),\n",
    "                    \"x1\": float(X[idx, 0]),\n",
    "                    \"x2\": float(X[idx, 1]),\n",
    "                    \"f1_in\": float(f1_in[i]),\n",
    "                    \"f1_shift\": float(f1_shift[i]),\n",
    "                    \"f1_drop\": float(f1_in[i] - f1_shift[i]),\n",
    "                    \"f2_in\": float(f2_in[i]),\n",
    "                    \"f2_shift\": float(f2_shift[i]),\n",
    "                })\n",
    "\n",
    "    # Figures\n",
    "    figdir = os.path.join(outdir, \"figures\")\n",
    "\n",
    "    # Pareto scatter: cost vs performance\n",
    "    plt.figure(figsize=(7.2, 5.4))\n",
    "    plt.scatter(f2, f1, s=18, label=\"All evaluated\")\n",
    "    plt.scatter(pareto_df[\"f2_cost\"], pareto_df[\"f1_perf\"], s=45, label=\"Pareto front\")\n",
    "    plt.xlabel(\"Cost (minimise)\")\n",
    "    plt.ylabel(\"Performance (maximise)\")\n",
    "    plt.title(\"Pareto Front (MOBO++)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"pareto_front.png\"), dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "    # Pareto ranks visualization: colour by rank using separate plot per rank groups (simple)\n",
    "    plt.figure(figsize=(7.2, 5.4))\n",
    "    for r in np.unique(ranks):\n",
    "        idx = np.where(ranks == r)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        plt.scatter(f2[idx], f1[idx], s=18, label=f\"rank {r}\")\n",
    "        if r >= 4:\n",
    "            break  # keep plot readable\n",
    "    plt.xlabel(\"Cost (minimise)\")\n",
    "    plt.ylabel(\"Performance (maximise)\")\n",
    "    plt.title(\"Non-dominated Sorting Levels (Pareto Ranks)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"pareto_ranks.png\"), dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "    # Hypervolume curve\n",
    "    plt.figure(figsize=(7.0, 4.8))\n",
    "    plt.plot(hv_df[\"iter\"], hv_df[\"hypervolume\"], marker=\"o\", linewidth=1.2)\n",
    "    plt.xlabel(\"Evaluations\")\n",
    "    plt.ylabel(\"Hypervolume (higher is better)\")\n",
    "    plt.title(\"Hypervolume Improvement over Evaluations\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"hypervolume_curve.png\"), dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "    # Hypervolume contribution bar (top Pareto points)\n",
    "    top_contrib = pareto_df.sort_values(\"hv_contribution\", ascending=False).head(min(12, len(pareto_df))).copy()\n",
    "    if len(top_contrib) > 0:\n",
    "        plt.figure(figsize=(8.2, 4.8))\n",
    "        plt.bar([str(int(i)) for i in top_contrib[\"index\"]], top_contrib[\"hv_contribution\"])\n",
    "        plt.xlabel(\"Pareto point index\")\n",
    "        plt.ylabel(\"Hypervolume contribution\")\n",
    "        plt.title(\"Most Valuable Pareto Points (Hypervolume Contribution)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(figdir, \"hv_contribution_top.png\"), dpi=220)\n",
    "        plt.close()\n",
    "\n",
    "    # Epsilon-constraint curve\n",
    "    plt.figure(figsize=(7.0, 4.8))\n",
    "    plt.plot(eps_df[\"epsilon_cost\"], eps_df[\"best_perf\"], marker=\"o\", linewidth=1.2)\n",
    "    plt.xlabel(\"Cost threshold ε\")\n",
    "    plt.ylabel(\"Best feasible performance\")\n",
    "    plt.title(\"Epsilon-Constraint Trade-off Curve\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"epsilon_constraint_curve.png\"), dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "    # Design space plot\n",
    "    plt.figure(figsize=(7.2, 5.4))\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=18, label=\"All evaluated\")\n",
    "    plt.scatter(pareto_df[\"x1\"], pareto_df[\"x2\"], s=45, label=\"Pareto designs\")\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(\"Design Space Coverage + Pareto Designs\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figdir, \"design_space_pareto.png\"), dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "    # Save metrics JSON\n",
    "    metrics = {\n",
    "        \"seed\": seed,\n",
    "        \"budget\": budget,\n",
    "        \"init_points\": init_points,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"n_candidates\": n_candidates,\n",
    "        \"explore_bonus\": explore_bonus,\n",
    "        \"hv_reference_point\": {\"f1_ref\": hv_ref[0], \"f2_ref\": hv_ref[1]},\n",
    "        \"hypervolume_final\": float(hv_final),\n",
    "        \"n_pareto_points\": int(len(pareto_df)),\n",
    "        \"best_perf_overall\": float(np.max(f1)),\n",
    "        \"min_cost_overall\": float(np.min(f2)),\n",
    "        \"pareto_best_perf\": float(np.max(pareto_df[\"f1_perf\"])) if len(pareto_df) else np.nan,\n",
    "        \"pareto_min_cost\": float(np.min(pareto_df[\"f2_cost\"])) if len(pareto_df) else np.nan,\n",
    "        \"ood_shift_check_top_pareto\": shift_report,\n",
    "        \"gp_kernels\": {\n",
    "            \"gp1_perf\": str(gp1.kernel_),\n",
    "            \"gp2_cost\": str(gp2.kernel_),\n",
    "        },\n",
    "    }\n",
    "    save_json(metrics, os.path.join(outdir, \"metrics_report.json\"))\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n=== MOBO++ (Pareto + Ranks + HV contribution + Epsilon constraint + Batch BO) ===\")\n",
    "    print(f\"Outputs saved to: {os.path.abspath(outdir)}\\n\")\n",
    "    print(\"Final hypervolume:\", float(hv_final))\n",
    "    print(\"Pareto front size:\", int(len(pareto_df)))\n",
    "    print(\"Best performance observed:\", float(np.max(f1)))\n",
    "    print(\"Minimum cost observed:\", float(np.min(f2)))\n",
    "    if len(pareto_df) > 0:\n",
    "        top = pareto_df.sort_values(\"hv_contribution\", ascending=False).head(1).iloc[0]\n",
    "        print(\"\\nTop Pareto point by hypervolume contribution:\")\n",
    "        print(\"  index:\", int(top[\"index\"]))\n",
    "        print(\"  x1,x2:\", float(top[\"x1\"]), float(top[\"x2\"]))\n",
    "        print(\"  perf:\", float(top[\"f1_perf\"]), \" cost:\", float(top[\"f2_cost\"]))\n",
    "        print(\"  hv contrib:\", float(top[\"hv_contribution\"]))\n",
    "\n",
    "    if shift_report is not None and len(shift_report) > 0:\n",
    "        print(\"\\nOOD shift check (top Pareto points, noiseless):\")\n",
    "        worst = max(shift_report, key=lambda d: d[\"f1_drop\"])\n",
    "        print(\"  worst perf drop:\", float(worst[\"f1_drop\"]), \"at index\", int(worst[\"index\"]))\n",
    "\n",
    "    print(\"\\nFigures ->\", os.path.join(outdir, \"figures\"))\n",
    "    print(\"Tables  ->\", os.path.join(outdir, \"tables\"), \"\\n\")\n",
    "\n",
    "\n",
    "def make_argparser() -> argparse.ArgumentParser:\n",
    "    p = argparse.ArgumentParser(description=\"UH MOBO++ PoC\")\n",
    "    p.add_argument(\"--outdir\", type=str, default=\"uh_mobo_plus\", help=\"Output directory\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "    p.add_argument(\"--budget\", type=int, default=100, help=\"Total evaluations (including init)\")\n",
    "    p.add_argument(\"--init_points\", type=int, default=16, help=\"Initial LHS points\")\n",
    "    p.add_argument(\"--batch_size\", type=int, default=4, help=\"Batch size per MOBO iteration\")\n",
    "    p.add_argument(\"--n_candidates\", type=int, default=12000, help=\"Random candidates per batch\")\n",
    "    p.add_argument(\"--explore_bonus\", type=float, default=0.18, help=\"Exploration strength\")\n",
    "    p.add_argument(\"--no_shift_test\", action=\"store_true\", help=\"Disable OOD shift check\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = make_argparser()\n",
    "    args, _ = parser.parse_known_args()  # Robust in Jupyter + terminal (ignores -f kernel.json)\n",
    "    run_mobo_plus(\n",
    "        outdir=args.outdir,\n",
    "        seed=args.seed,\n",
    "        budget=args.budget,\n",
    "        init_points=args.init_points,\n",
    "        batch_size=args.batch_size,\n",
    "        n_candidates=args.n_candidates,\n",
    "        explore_bonus=args.explore_bonus,\n",
    "        shift_test=(not args.no_shift_test),\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14400973-092a-4b9b-b34b-d4d43c6f6ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
